{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# RQ-VAE Training\n",
    "\n",
    "Train a Residual Quantized Variational AutoEncoder (RQ-VAE) to learn semantic IDs for items in a catalogue.\n",
    "\n",
    "**Environment:** RunPod Jupyter with GPU\n",
    "\n",
    "**What this does:**\n",
    "1. Load item catalogue (JSONL format)\n",
    "2. Generate embeddings using a pretrained embedding model\n",
    "3. Train RQ-VAE to compress embeddings into discrete codes\n",
    "4. Save trained model (optionally to W&B artifacts)\n",
    "\n",
    "**Outputs:**\n",
    "- `models/rqvae_model.pt` - Trained RQ-VAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to project root if needed\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if os.path.abspath(\".\").endswith(\"notebooks\"):\n",
    "    repo_root = os.path.abspath(\"..\")\n",
    "    print(f\"Current directory: {os.getcwd()}, changing to {repo_root}\")\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.insert(0, repo_root)\n",
    "    os.chdir(repo_root)\n",
    "else:\n",
    "    print(f\"Already in project root: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel # Always import unsloth first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from src.rqvae import (\n",
    "    train,\n",
    "    RqvaeTrainConfig,\n",
    "    SemanticRQVAE,\n",
    "    SemanticRQVAEConfig,\n",
    ")\n",
    "\n",
    "print(\"Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All training parameters are consolidated into `RqvaeTrainConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load configuration from YAML\n",
    "with open(\"notebooks/rqvae_config.yaml\") as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "config = RqvaeTrainConfig(**config_dict)\n",
    "\n",
    "# Create output directories\n",
    "Path(\"models\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Catalogue: {config.catalogue_path}\")\n",
    "print(f\"  Embedding model: {config.embedding_model}\")\n",
    "print(f\"  Codebook: {config.num_quantizers} levels x {config.codebook_size} codes\")\n",
    "print(f\"  Total semantic ID space: {config.codebook_size ** config.num_quantizers:,} unique IDs\")\n",
    "print(f\"  Output model: {config.model_save_path}\")\n",
    "print(f\"  W&B project: {config.wandb_project}\")\n",
    "print(f\"  Log W&B artifacts: {config.log_wandb_artifacts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Train RQ-VAE\n",
    "\n",
    "The `train()` function handles the complete training lifecycle:\n",
    "1. Initialize W&B (if project provided)\n",
    "2. Load catalogue and generate/cache embeddings\n",
    "3. Split dataset into train/val\n",
    "4. Train the model with Lightning\n",
    "5. Evaluate and compute final metrics\n",
    "6. Log summary metrics to W&B\n",
    "7. Clean up W&B run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run end-to-end training\n",
    "result = train(config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Review Results\n",
    "\n",
    "The `train()` function returns a `TrainResult` containing:\n",
    "- `model`: Trained SemanticRQVAE model\n",
    "- `config`: Model configuration\n",
    "- `metrics`: Dictionary of final evaluation metrics\n",
    "- `semantic_ids`: Mapping of item_id -> semantic_id string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final metrics\n",
    "print(\"=== Final Metrics ===\")\n",
    "print(f\"Average perplexity: {result.metrics['avg_perplexity']:.2f} / {config.codebook_size} (max)\")\n",
    "print(f\"Average usage: {result.metrics['avg_usage']*100:.1f}%\")\n",
    "print(f\"Total items: {result.metrics['total_items']}\")\n",
    "print(f\"Unique semantic IDs: {result.metrics['unique_semantic_ids']}\")\n",
    "print(f\"Collision rate: {result.metrics['collision_rate']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nPer-level breakdown:\")\n",
    "for q in range(config.num_quantizers):\n",
    "    perp = result.metrics[f'level_{q}_perplexity']\n",
    "    usage = result.metrics[f'level_{q}_usage'] * 100\n",
    "    print(f\"  Level {q}: perplexity={perp:.1f}, usage={usage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample semantic IDs\n",
    "print(\"=== Sample Semantic IDs ===\")\n",
    "for i, (item_id, sem_id) in enumerate(list(result.semantic_ids.items())[:5]):\n",
    "    print(f\"  {item_id}: {sem_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 5. Test Loading from W&B Artifact\n",
    "\n",
    "Verify that we can load the trained model from the W&B artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "if config.log_wandb_artifacts:\n",
    "    # Initialize a new wandb run to download the artifact\n",
    "    wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        job_type=\"artifact-verification\",\n",
    "    )\n",
    "    \n",
    "    # Download the artifact we just logged\n",
    "    artifact = wandb.use_artifact(f\"{config.artifact_name}:latest\")\n",
    "    artifact_dir = artifact.download()\n",
    "    \n",
    "    print(f\"Downloaded artifact to: {artifact_dir}\")\n",
    "    print(f\"Artifact metadata: {artifact.metadata}\")\n",
    "    \n",
    "    # Load the model from artifact\n",
    "    checkpoint = torch.load(f\"{artifact_dir}/rqvae_model.pt\")\n",
    "    artifact_config = SemanticRQVAEConfig(**checkpoint[\"config\"])\n",
    "    artifact_model = SemanticRQVAE(artifact_config)\n",
    "    artifact_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    artifact_model.eval()\n",
    "    \n",
    "    print(f\"\\nLoaded model from W&B artifact: {config.artifact_name}:latest\")\n",
    "    print(f\"  Config: {checkpoint['config']}\")\n",
    "    \n",
    "    wandb.finish()\n",
    "else:\n",
    "    print(\"Skipping artifact verification (log_wandb_artifacts=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.log_wandb_artifacts:\n",
    "    # Verify artifact model produces same outputs as trained model\n",
    "    from src.rqvae import ItemEmbeddingDataset\n",
    "    \n",
    "    # Load embeddings to test\n",
    "    dataset = ItemEmbeddingDataset.from_embeddings_file(config.embeddings_cache_path)\n",
    "    test_embeddings = dataset.embeddings[:5]\n",
    "    \n",
    "    # Get device of trained model\n",
    "    device = next(result.model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get IDs from trained model\n",
    "        original_ids = result.model.semantic_id_to_string(\n",
    "            result.model.get_semantic_ids(test_embeddings.to(device))\n",
    "        )\n",
    "        \n",
    "        # Get IDs from artifact model (on CPU)\n",
    "        artifact_ids = artifact_model.semantic_id_to_string(\n",
    "            artifact_model.get_semantic_ids(test_embeddings)\n",
    "        )\n",
    "    \n",
    "    print(\"=== Artifact Verification ===\")\n",
    "    all_match = True\n",
    "    for i, (orig, art) in enumerate(zip(original_ids, artifact_ids)):\n",
    "        match = orig == art\n",
    "        all_match = all_match and match\n",
    "        status = \"OK\" if match else \"MISMATCH\"\n",
    "        print(f\"  [{status}] {orig}\")\n",
    "    \n",
    "    if all_match:\n",
    "        print(\"\\nArtifact model produces identical outputs to trained model\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: Artifact model outputs differ from trained model!\")\n",
    "else:\n",
    "    print(\"Skipping artifact verification (log_wandb_artifacts=False)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
