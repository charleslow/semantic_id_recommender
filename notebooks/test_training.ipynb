{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic ID Recommender - Training Test\n",
    "\n",
    "This notebook tests the full training pipeline in Google Colab.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 is sufficient)\n",
    "- ~10 minutes for full test\n",
    "\n",
    "**What this tests:**\n",
    "1. RQ-VAE training (semantic ID learning)\n",
    "2. Training data generation\n",
    "3. LLM fine-tuning with Unsloth\n",
    "4. Inference test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv\n",
    "!pip install uv\n",
    "\n",
    "# Clone private repo (skip if already exists)\n",
    "import os\n",
    "if not os.path.exists(\"semantic_id_recommender\"):\n",
    "    !git clone https://github.com/charleslow/semantic_id_recommender.git\n",
    "else:\n",
    "    print(\"✓ Repo already exists\")\n",
    "    \n",
    "%cd semantic_id_recommender\n",
    "!git pull --ff-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (let uv resolve versions compatible with Colab's environment)\n",
    "!uv pip install --system \\\n",
    "    torch torchvision torchaudio \\\n",
    "    transformers datasets accelerate \\\n",
    "    vector-quantize-pytorch sentence-transformers \\\n",
    "    lightning wandb omegaconf einops tqdm rich \\\n",
    "    pydantic unsloth psutil\n",
    "\n",
    "# Fix protobuf version conflict\n",
    "!uv pip install --system \"protobuf>=3.20,<5\"\n",
    "\n",
    "# Note: We skip using uv.lock because Colab has pre-installed packages\n",
    "# that may conflict with locked versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import from Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add repo root to Python path for imports\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "# Import from the cloned package\n",
    "import torch\n",
    "from src.rqvae.model import SemanticRQVAE, SemanticRQVAEConfig\n",
    "from src.rqvae.trainer import RQVAETrainer\n",
    "from src.rqvae.dataset import ItemEmbeddingDataset\n",
    "\n",
    "print(\"✓ Package imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "items = []\n",
    "with open(\"data/ag_news_500.jsonl\") as f:\n",
    "    for line in f:\n",
    "        items.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(items)} items\")\n",
    "print(f\"\\nKeys: {items[0].keys()}\")\n",
    "print(f\"\\nSample items:\")\n",
    "for item in items[:5]:\n",
    "    print(f\"  [{item['category']}] {item['item_id']}: {item['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the package's ItemEmbeddingDataset to generate embeddings\n",
    "# Using GTE-Qwen2 - a powerful Qwen-based embedding model (1536 dim)\n",
    "dataset = ItemEmbeddingDataset.from_catalogue(\n",
    "    catalogue_path=\"data/ag_news_500.jsonl\",\n",
    "    embedding_model=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    cache_path=\"data/embeddings_qwen3_0.6b.pt\",  # New cache for different model\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Number of items: {len(dataset)}\")\n",
    "print(f\"  Embedding dim: {dataset.embeddings.shape[1]}\")\n",
    "print(f\"  Item IDs (first 5): {dataset.item_ids[:5]}\")\n",
    "\n",
    "# Keep embeddings tensor for later use\n",
    "embeddings = dataset.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Category distribution\n",
    "categories = [item['category'] for item in items]\n",
    "cat_counts = Counter(categories)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart of categories\n",
    "ax1 = axes[0]\n",
    "ax1.bar(cat_counts.keys(), cat_counts.values(), color=['#2ecc71', '#3498db', '#e74c3c', '#9b59b6'])\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Distribution by Category')\n",
    "for i, (cat, count) in enumerate(cat_counts.items()):\n",
    "    ax1.text(i, count + 2, str(count), ha='center')\n",
    "\n",
    "# Content length distribution\n",
    "content_lengths = [len(item['content']) for item in items]\n",
    "ax2 = axes[1]\n",
    "ax2.hist(content_lengths, bins=30, color='#3498db', edgecolor='white')\n",
    "ax2.set_xlabel('Content Length (chars)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distribution of Article Lengths')\n",
    "ax2.axvline(sum(content_lengths)/len(content_lengths), color='red', linestyle='--', label=f'Mean: {sum(content_lengths)//len(content_lengths)}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total articles: {len(items)}\")\n",
    "print(f\"  Categories: {dict(cat_counts)}\")\n",
    "print(f\"  Avg content length: {sum(content_lengths)//len(content_lengths)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train RQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb for the entire experiment\n",
    "wandb.init(\n",
    "    project=\"semantic-id-recommender\",\n",
    "    name=\"full-pipeline-test\",\n",
    "    config={\n",
    "        \"embedding_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"num_items\": len(items),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "config = SemanticRQVAEConfig(\n",
    "    embedding_dim=1024,  # Match embedding model output dimension\n",
    "    hidden_dim=256,      # Larger hidden dim for bigger embeddings\n",
    "    codebook_size=32,\n",
    "    num_quantizers=3,\n",
    "    threshold_ema_dead_code=1,\n",
    ")\n",
    "\n",
    "# Log RQ-VAE config to wandb\n",
    "wandb.config.update({\n",
    "    \"rqvae_embedding_dim\": config.embedding_dim,\n",
    "    \"rqvae_hidden_dim\": config.hidden_dim,\n",
    "    \"rqvae_codebook_size\": config.codebook_size,\n",
    "    \"rqvae_num_quantizers\": config.num_quantizers,\n",
    "})\n",
    "\n",
    "trainer_module = RQVAETrainer(config=config, learning_rate=1e-3)\n",
    "\n",
    "# Split dataset into train/val (90/10)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# Create WandbLogger for Lightning\n",
    "wandb_logger = WandbLogger(experiment=wandb.run)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1,\n",
    "    logger=wandb_logger,  # Log to wandb\n",
    ")\n",
    "trainer.fit(trainer_module, train_loader, val_loader)\n",
    "print(\"✓ RQ-VAE training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training diagnostics\n",
    "print(\"=== Training Diagnostics ===\\n\")\n",
    "\n",
    "# Get logged metrics from trainer\n",
    "logged_metrics = trainer.logged_metrics\n",
    "print(\"Final logged metrics:\")\n",
    "for key, value in sorted(logged_metrics.items()):\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Compute codebook stats on full dataset\n",
    "model = trainer_module.model\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_indices = model.get_semantic_ids(embeddings.to(device))\n",
    "    stats = model.compute_codebook_stats(all_indices)\n",
    "\n",
    "print(f\"\\n=== Codebook Statistics (full dataset) ===\")\n",
    "print(f\"Average perplexity: {stats['avg_perplexity']:.2f} / {config.codebook_size} (max)\")\n",
    "print(f\"Average usage: {stats['avg_usage']*100:.1f}%\")\n",
    "print(f\"\\nPer-level breakdown:\")\n",
    "for q in range(config.num_quantizers):\n",
    "    perp = stats['perplexity_per_level'][q].item()\n",
    "    usage = stats['usage_per_level'][q].item() * 100\n",
    "    print(f\"  Level {q}: perplexity={perp:.1f}, usage={usage:.1f}%\")\n",
    "\n",
    "# Check for semantic ID collisions\n",
    "unique_ids = len(set(model.semantic_id_to_string(all_indices)))\n",
    "print(f\"\\n=== Semantic ID Quality ===\")\n",
    "print(f\"Total items: {len(embeddings)}\")\n",
    "print(f\"Unique semantic IDs: {unique_ids}\")\n",
    "print(f\"Collision rate: {(1 - unique_ids/len(embeddings))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate semantic IDs for all items using the trained model\n",
    "model = trainer_module.model\n",
    "model.eval()\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "with torch.no_grad():\n",
    "    indices = model.get_semantic_ids(embeddings.to(device))\n",
    "    semantic_strings = model.semantic_id_to_string(indices)\n",
    "\n",
    "# Create mapping\n",
    "item_to_semantic = {}\n",
    "semantic_to_item = {}\n",
    "\n",
    "for i, item in enumerate(items):\n",
    "    item_id = item[\"item_id\"]  # AG News uses item_id\n",
    "    sem_id = semantic_strings[i]\n",
    "    item_to_semantic[item_id] = {\n",
    "        \"codes\": indices[i].cpu().tolist(),\n",
    "        \"semantic_id\": sem_id,\n",
    "    }\n",
    "    semantic_to_item[sem_id] = item_id\n",
    "\n",
    "# Save mapping\n",
    "mapping = {\n",
    "    \"item_to_semantic\": item_to_semantic,\n",
    "    \"semantic_to_item\": semantic_to_item,\n",
    "    \"config\": {\"num_quantizers\": config.num_quantizers, \"codebook_size\": config.codebook_size}\n",
    "}\n",
    "\n",
    "with open(\"data/semantic_ids.json\", \"w\") as f:\n",
    "    json.dump(mapping, f, indent=2)\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample semantic IDs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {items[i]['title'][:30]:30} -> {semantic_strings[i]}\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(semantic_to_item)} unique semantic IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine embedding similarity vs semantic ID similarity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Compute pairwise cosine similarities\n",
    "normed = F.normalize(embeddings, dim=1)\n",
    "similarities = normed @ normed.T\n",
    "\n",
    "# Mask out diagonal and near-duplicates (sim > 0.99)\n",
    "similarities_filtered = similarities.clone()\n",
    "similarities_filtered.fill_diagonal_(-1)\n",
    "mask = similarities_filtered > 0.99\n",
    "similarities_filtered[mask] = -1  # Exclude near-duplicates\n",
    "\n",
    "# Find most similar pair (excluding duplicates)\n",
    "max_idx = similarities_filtered.argmax()\n",
    "i_close, j_close = max_idx // len(items), max_idx % len(items)\n",
    "\n",
    "# Find most dissimilar pair\n",
    "similarities_for_min = similarities.clone()\n",
    "similarities_for_min.fill_diagonal_(2)  # Exclude self\n",
    "min_idx = similarities_for_min.argmin()\n",
    "i_far, j_far = min_idx // len(items), min_idx % len(items)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLOSEST PAIR (high embedding similarity, excluding duplicates)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Cosine similarity: {similarities[i_close, j_close]:.4f}\")\n",
    "print(f\"\\nItem A [{items[i_close]['category']}]:\")\n",
    "print(f\"  Title: {items[i_close]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[i_close]}\")\n",
    "print(f\"\\nItem B [{items[j_close]['category']}]:\")\n",
    "print(f\"  Title: {items[j_close]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[j_close]}\")\n",
    "\n",
    "# Check how many codes match\n",
    "codes_a = indices[i_close].tolist()\n",
    "codes_b = indices[j_close].tolist()\n",
    "matching = sum(a == b for a, b in zip(codes_a, codes_b))\n",
    "print(f\"\\nCodes A: {codes_a}\")\n",
    "print(f\"Codes B: {codes_b}\")\n",
    "print(f\"Matching codes: {matching}/{len(codes_a)} ✓\" if matching > 0 else f\"Matching codes: {matching}/{len(codes_a)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FARTHEST PAIR (low embedding similarity)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Cosine similarity: {similarities[i_far, j_far]:.4f}\")\n",
    "print(f\"\\nItem A [{items[i_far]['category']}]:\")\n",
    "print(f\"  Title: {items[i_far]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[i_far]}\")\n",
    "print(f\"\\nItem B [{items[j_far]['category']}]:\")\n",
    "print(f\"  Title: {items[j_far]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[j_far]}\")\n",
    "\n",
    "codes_a = indices[i_far].tolist()\n",
    "codes_b = indices[j_far].tolist()\n",
    "matching = sum(a == b for a, b in zip(codes_a, codes_b))\n",
    "print(f\"\\nCodes A: {codes_a}\")\n",
    "print(f\"Codes B: {codes_b}\")\n",
    "print(f\"Matching codes: {matching}/{len(codes_a)}\")\n",
    "\n",
    "# Check for duplicates in dataset\n",
    "dup_count = (similarities > 0.999).sum().item() - len(items)  # Subtract diagonal\n",
    "print(f\"\\n⚠️  Found {dup_count//2} duplicate pairs in dataset (cosine sim > 0.999)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data using the package utilities\n",
    "from src.llm.data import format_for_chat\n",
    "\n",
    "# Create items with semantic IDs for training\n",
    "items_with_sem_ids = []\n",
    "for item in items:\n",
    "    item_copy = item.copy()\n",
    "    item_copy[\"semantic_id\"] = item_to_semantic[item[\"item_id\"]][\"semantic_id\"]\n",
    "    items_with_sem_ids.append(item_copy)\n",
    "\n",
    "# Generate training examples\n",
    "from src.llm.data import generate_training_examples\n",
    "raw_examples = generate_training_examples(\n",
    "    items_with_sem_ids, \n",
    "    num_examples_per_item=10,\n",
    "    query_templates=[\n",
    "        # Direct title queries\n",
    "        \"{title}\",\n",
    "        \"Find me news about {title}\",\n",
    "        \"I'm looking for articles on {title}\",\n",
    "        \"Show me {title}\",\n",
    "        \"Search for {title}\",\n",
    "        # Category-based queries\n",
    "        \"News about {category}: {title}\",\n",
    "        \"{category} news: {title}\",\n",
    "        \"Latest {category} article about {title}\",\n",
    "        # Question formats\n",
    "        \"What's the news about {title}?\",\n",
    "        \"Any articles on {title}?\",\n",
    "        \"Do you have news about {title}?\",\n",
    "        # Recommendation style\n",
    "        \"Recommend articles about {title}\",\n",
    "        \"Suggest news on {title}\",\n",
    "        # Short/casual queries\n",
    "        \"news {title}\",\n",
    "        \"article {title}\",\n",
    "        \"{category} {title}\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Format for chat and split\n",
    "import random\n",
    "formatted_examples = format_for_chat(raw_examples)\n",
    "random.shuffle(formatted_examples)\n",
    "split_idx = int(len(formatted_examples) * 0.9)\n",
    "train_examples = formatted_examples[:split_idx]\n",
    "val_examples = formatted_examples[split_idx:]\n",
    "\n",
    "print(f\"✓ Generated {len(train_examples)} train, {len(val_examples)} val examples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Query: {train_examples[0]['messages'][1]['content'][:50]}...\")\n",
    "print(f\"  Target: {train_examples[0]['messages'][2]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tune LLM with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update wandb config for LLM training (wandb already initialized)\n",
    "wandb.config.update({\n",
    "    \"base_model\": \"unsloth/SmolLM2-135M-Instruct\",\n",
    "    \"num_train_examples\": len(train_examples),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "\n",
    "# Use SmolLM - very small model (135M parameters)\n",
    "BASE_MODEL = \"unsloth/SmolLM2-135M-Instruct\"\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "print(f\"Loading {BASE_MODEL}...\")\n",
    "llm_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add semantic ID tokens using package utility\n",
    "from src.llm.finetune import add_semantic_tokens\n",
    "\n",
    "tokenizer = add_semantic_tokens(tokenizer, config.num_quantizers, config.codebook_size)\n",
    "llm_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"✓ Added {config.num_quantizers * config.codebook_size} semantic tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "llm_model = FastLanguageModel.get_peft_model(\n",
    "    llm_model,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"✓ LoRA adapters added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset using package formatting function\n",
    "def format_single_example(messages):\n",
    "    \"\"\"Format messages into prompt string.\"\"\"\n",
    "    text = \"\"\n",
    "    for msg in messages:\n",
    "        role, content = msg[\"role\"], msg[\"content\"]\n",
    "        if role == \"system\":\n",
    "            text += f\"<|system|>\\n{content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            text += f\"<|user|>\\n{content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            text += f\"<|assistant|>\\n{content}\\n\"\n",
    "    return text\n",
    "\n",
    "train_texts = [{\"text\": format_single_example(ex[\"messages\"])} for ex in train_examples]\n",
    "val_texts = [{\"text\": format_single_example(ex[\"messages\"])} for ex in val_examples]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_texts)\n",
    "val_dataset = Dataset.from_list(val_texts)\n",
    "\n",
    "print(f\"✓ Dataset prepared: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
    "print(f\"Sample: {train_texts[0]['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: Unsloth's compiled cache needs psutil in builtins\n",
    "import psutil\n",
    "import builtins\n",
    "builtins.psutil = psutil\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/llm\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",      # Enable evaluation\n",
    "    eval_steps=50,              # Evaluate every 50 steps\n",
    "    fp16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,   # Add validation dataset\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(llm_model)\n",
    "from src.llm.finetune import SemanticIDGenerator\n",
    "\n",
    "generator = SemanticIDGenerator(\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    num_quantizers=config.num_quantizers,\n",
    "    codebook_size=config.codebook_size,\n",
    "    use_constrained_decoding=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Inference mode ready with constrained decoding\")\n",
    "print(f\"  Semantic tokens: {config.num_quantizers * config.codebook_size} total\")\n",
    "print(f\"  Output format: {config.num_quantizers} tokens (one per quantizer level)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries with beam search - show top 10 candidates and valid matches\n",
    "test_queries = [\n",
    "    \"News about stock market and business\",\n",
    "    \"Sports football game results\",\n",
    "    \"Technology and science discoveries\",\n",
    "    \"World news international politics\",\n",
    "]\n",
    "\n",
    "NUM_BEAMS = 10\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFERENCE TESTS (Beam Search)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_table = []\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    # Get top candidates via beam search\n",
    "    candidates = generator.generate_beam(query, num_beams=NUM_BEAMS, num_return_sequences=NUM_BEAMS)\n",
    "    \n",
    "    print(f\"\\nTop {len(candidates)} candidates:\")\n",
    "    valid_count = 0\n",
    "    for rank, (sem_id, score) in enumerate(candidates, 1):\n",
    "        if sem_id in semantic_to_item:\n",
    "            item_id = semantic_to_item[sem_id]\n",
    "            item = next(i for i in items if i[\"item_id\"] == item_id)\n",
    "            print(f\"  {rank}. ✓ {sem_id} (score: {score:.2f})\")\n",
    "            print(f\"     [{item['category']}] {item['title'][:55]}...\")\n",
    "            valid_count += 1\n",
    "            results_table.append([query, rank, sem_id, score, item['category'], item['title'][:50], True])\n",
    "        else:\n",
    "            print(f\"  {rank}. ✗ {sem_id} (score: {score:.2f}) - not in catalogue\")\n",
    "            results_table.append([query, rank, sem_id, score, \"\", \"\", False])\n",
    "    \n",
    "    print(f\"\\n  Valid matches: {valid_count}/{len(candidates)}\")\n",
    "\n",
    "# Log results to wandb\n",
    "wandb.log({\n",
    "    \"inference_results\": wandb.Table(\n",
    "        columns=[\"query\", \"rank\", \"semantic_id\", \"score\", \"category\", \"title\", \"valid\"],\n",
    "        data=results_table\n",
    "    )\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ All tests complete!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested:\n",
    "\n",
    "1. **RQ-VAE Training** - ✓ Model learns to encode items as discrete codes\n",
    "2. **Semantic ID Generation** - ✓ Each item gets a unique semantic ID\n",
    "3. **Training Data Creation** - ✓ Query → Semantic ID pairs generated\n",
    "4. **LLM Fine-tuning** - ✓ Model learns to predict semantic IDs\n",
    "5. **Inference** - ✓ Model generates semantic IDs for new queries\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "For production use:\n",
    "- Use larger model (Qwen3-4B or Ministral-3B)\n",
    "- Train for more epochs (3-5)\n",
    "- Use full catalogue\n",
    "- Deploy to Modal for serverless inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
