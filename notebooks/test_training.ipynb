{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic ID Recommender - Training Test\n",
    "\n",
    "This notebook tests the full training pipeline in Google Colab.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 is sufficient)\n",
    "- ~10 minutes for full test\n",
    "\n",
    "**What this tests:**\n",
    "1. RQ-VAE training (semantic ID learning)\n",
    "2. Training data generation\n",
    "3. LLM fine-tuning with Unsloth\n",
    "4. Inference test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv\n",
    "!pip install uv\n",
    "\n",
    "# Clone private repo (skip if already exists)\n",
    "import os\n",
    "if not os.path.exists(\"semantic_id_recommender\"):\n",
    "    !git clone https://github.com/charleslow/semantic_id_recommender.git\n",
    "else:\n",
    "    print(\"✓ Repo already exists\")\n",
    "    \n",
    "%cd semantic_id_recommender\n",
    "!git pull --ff-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (let uv resolve versions compatible with Colab's environment)\n",
    "!uv pip install --system \\\n",
    "    torch torchvision torchaudio \\\n",
    "    transformers datasets accelerate \\\n",
    "    vector-quantize-pytorch sentence-transformers \\\n",
    "    lightning wandb omegaconf einops tqdm rich \\\n",
    "    pydantic\n",
    "\n",
    "# Fix protobuf version conflict\n",
    "!uv pip install --system \"protobuf>=3.20,<5\"\n",
    "\n",
    "# Note: We skip using uv.lock because Colab has pre-installed packages\n",
    "# that may conflict with locked versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import from Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add repo root to Python path for imports\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "# Import from the cloned package\n",
    "import torch\n",
    "from src.rqvae.model import SemanticRQVAE, SemanticRQVAEConfig\n",
    "from src.rqvae.trainer import RQVAETrainer\n",
    "from src.rqvae.dataset import ItemEmbeddingDataset\n",
    "\n",
    "print(\"✓ Package imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rqvae.dataset import create_dummy_catalogue, ItemEmbeddingDataset\n",
    "import json\n",
    "\n",
    "NUM_ITEMS = 500\n",
    "\n",
    "# Create dummy catalogue\n",
    "create_dummy_catalogue(num_items=NUM_ITEMS, output_path=\"data/catalogue.json\")\n",
    "\n",
    "# Load and inspect\n",
    "with open(\"data/catalogue.json\") as f:\n",
    "    catalogue = json.load(f)\n",
    "    items = catalogue[\"items\"]\n",
    "\n",
    "print(f\"\\nSample items:\")\n",
    "for item in items[:5]:\n",
    "    print(f\"  {item['id']}: {item['title']} - ${item['price']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the package's ItemEmbeddingDataset to generate embeddings\n",
    "dataset = ItemEmbeddingDataset.from_catalogue(\n",
    "    catalogue_path=\"data/catalogue.json\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_path=\"data/embeddings.pt\",  # Cache for faster reruns\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Number of items: {len(dataset)}\")\n",
    "print(f\"  Embedding dim: {dataset.embeddings.shape[1]}\")\n",
    "print(f\"  Item IDs (first 5): {dataset.item_ids[:5]}\")\n",
    "\n",
    "# Keep embeddings tensor for later use\n",
    "embeddings = dataset.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train RQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Config for quick test\n",
    "config = SemanticRQVAEConfig(\n",
    "    embedding_dim=384,\n",
    "    hidden_dim=256,\n",
    "    codebook_size=64,  # Smaller for quick test\n",
    "    num_quantizers=3,\n",
    ")\n",
    "\n",
    "# Create Lightning trainer module\n",
    "trainer_module = RQVAETrainer(config=config, learning_rate=1e-3)\n",
    "\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(embeddings)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Train with Lightning\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "trainer.fit(trainer_module, train_loader, val_loader)\n",
    "print(\"✓ RQ-VAE training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate semantic IDs for all items using the trained model\n",
    "model = trainer_module.model\n",
    "model.eval()\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "with torch.no_grad():\n",
    "    indices = model.get_semantic_ids(embeddings.to(device))\n",
    "    semantic_strings = model.semantic_id_to_string(indices)\n",
    "\n",
    "# Create mapping\n",
    "item_to_semantic = {}\n",
    "semantic_to_item = {}\n",
    "\n",
    "for i, item in enumerate(items):\n",
    "    item_id = item[\"id\"]\n",
    "    sem_id = semantic_strings[i]\n",
    "    item_to_semantic[item_id] = {\n",
    "        \"codes\": indices[i].cpu().tolist(),\n",
    "        \"semantic_id\": sem_id,\n",
    "    }\n",
    "    semantic_to_item[sem_id] = item_id\n",
    "\n",
    "# Save mapping\n",
    "mapping = {\n",
    "    \"item_to_semantic\": item_to_semantic,\n",
    "    \"semantic_to_item\": semantic_to_item,\n",
    "    \"config\": {\"num_quantizers\": config.num_quantizers, \"codebook_size\": config.codebook_size}\n",
    "}\n",
    "\n",
    "with open(\"data/semantic_ids.json\", \"w\") as f:\n",
    "    json.dump(mapping, f, indent=2)\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample semantic IDs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {items[i]['title'][:30]:30} -> {semantic_strings[i]}\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(semantic_to_item)} unique semantic IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate query -> semantic ID training pairs\n",
    "query_templates = [\n",
    "    \"Find me a {title}\",\n",
    "    \"I'm looking for {title}\",\n",
    "    \"Recommend a {title}\",\n",
    "    \"{title}\",\n",
    "    \"{description}\",\n",
    "]\n",
    "\n",
    "training_examples = []\n",
    "system_prompt = \"You are a recommendation system. Given a user query, output the semantic ID of the most relevant item.\"\n",
    "\n",
    "for item in items:\n",
    "    item_id = item[\"id\"]\n",
    "    sem_id = item_to_semantic[item_id][\"semantic_id\"]\n",
    "    \n",
    "    for template in random.sample(query_templates, 2):\n",
    "        query = template.format(title=item[\"title\"], description=item[\"description\"])\n",
    "        \n",
    "        training_examples.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "                {\"role\": \"assistant\", \"content\": sem_id},\n",
    "            ],\n",
    "            \"item_id\": item_id,\n",
    "        })\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(training_examples)\n",
    "split_idx = int(len(training_examples) * 0.9)\n",
    "train_examples = training_examples[:split_idx]\n",
    "val_examples = training_examples[split_idx:]\n",
    "\n",
    "# Save\n",
    "with open(\"data/train.jsonl\", \"w\") as f:\n",
    "    for ex in train_examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "with open(\"data/val.jsonl\", \"w\") as f:\n",
    "    for ex in val_examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Generated {len(train_examples)} train, {len(val_examples)} val examples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Query: {train_examples[0]['messages'][1]['content'][:50]}...\")\n",
    "print(f\"  Target: {train_examples[0]['messages'][2]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tune LLM with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "# Disable wandb for this test\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Use a small model for testing\n",
    "BASE_MODEL = \"unsloth/Qwen2.5-0.5B\"  # Very small for quick test\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "print(f\"Loading {BASE_MODEL}...\")\n",
    "llm_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add semantic ID tokens\n",
    "semantic_tokens = []\n",
    "for q in range(config.num_quantizers):\n",
    "    for c in range(config.codebook_size):\n",
    "        semantic_tokens.append(f\"[SEM_{q}_{c}]\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": semantic_tokens})\n",
    "llm_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"✓ Added {len(semantic_tokens)} semantic tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "llm_model = FastLanguageModel.get_peft_model(\n",
    "    llm_model,\n",
    "    r=8,  # Small for quick test\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"✓ LoRA adapters added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def formatting_func(examples):\n",
    "    output_texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        text = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            if role == \"system\":\n",
    "                text += f\"<|system|>\\n{content}\\n\"\n",
    "            elif role == \"user\":\n",
    "                text += f\"<|user|>\\n{content}\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                text += f\"<|assistant|>\\n{content}\\n\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "val_dataset = Dataset.from_list(val_examples)\n",
    "\n",
    "print(f\"✓ Dataset prepared: {len(train_dataset)} train, {len(val_dataset)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (quick test settings)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/llm\",\n",
    "    num_train_epochs=1,  # Just 1 epoch for test\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",  # Don't save for test\n",
    "    eval_strategy=\"no\",\n",
    "    fp16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to inference mode\n",
    "FastLanguageModel.for_inference(llm_model)\n",
    "\n",
    "def generate_recommendation(query: str) -> str:\n",
    "    system_prompt = \"You are a recommendation system. Given a user query, output the semantic ID of the most relevant item.\"\n",
    "    prompt = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{query}\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm_model.device)\n",
    "    \n",
    "    outputs = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=32,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # Extract part after <|assistant|>\n",
    "    if \"<|assistant|>\" in generated:\n",
    "        result = generated.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        result = generated[len(prompt):].strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Inference mode ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Find me a premium electronics widget\",\n",
    "    \"I need a budget sports accessory\",\n",
    "    \"Recommend a luxury home device\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in test_queries:\n",
    "    result = generate_recommendation(query)\n",
    "    \n",
    "    # Try to find the item\n",
    "    import re\n",
    "    sem_id_match = re.search(r\"(\\[SEM_\\d+_\\d+\\])+\", result)\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Generated: {result[:100]}\")\n",
    "    \n",
    "    if sem_id_match:\n",
    "        sem_id = sem_id_match.group(0)\n",
    "        if sem_id in semantic_to_item:\n",
    "            item_id = semantic_to_item[sem_id]\n",
    "            item = next(i for i in items if i[\"id\"] == item_id)\n",
    "            print(f\"Matched: {item['title']}\")\n",
    "        else:\n",
    "            print(f\"Semantic ID not in catalogue: {sem_id}\")\n",
    "    else:\n",
    "        print(\"No valid semantic ID found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ All tests complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested:\n",
    "\n",
    "1. **RQ-VAE Training** - ✓ Model learns to encode items as discrete codes\n",
    "2. **Semantic ID Generation** - ✓ Each item gets a unique semantic ID\n",
    "3. **Training Data Creation** - ✓ Query → Semantic ID pairs generated\n",
    "4. **LLM Fine-tuning** - ✓ Model learns to predict semantic IDs\n",
    "5. **Inference** - ✓ Model generates semantic IDs for new queries\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "For production use:\n",
    "- Use larger model (Qwen3-4B or Ministral-3B)\n",
    "- Train for more epochs (3-5)\n",
    "- Use full catalogue\n",
    "- Deploy to Modal for serverless inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
