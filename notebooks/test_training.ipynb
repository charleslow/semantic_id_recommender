{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic ID Recommender - Training Test\n",
    "\n",
    "This notebook tests the full training pipeline in Google Colab.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 is sufficient)\n",
    "- ~10 minutes for full test\n",
    "\n",
    "**What this tests:**\n",
    "1. RQ-VAE training (semantic ID learning)\n",
    "2. Training data generation\n",
    "3. LLM fine-tuning with Unsloth\n",
    "4. Inference test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv\n",
    "!pip install uv\n",
    "\n",
    "# Clone private repo (skip if already exists)\n",
    "import os\n",
    "if not os.path.exists(\"semantic_id_recommender\"):\n",
    "    !git clone https://github.com/charleslow/semantic_id_recommender.git\n",
    "else:\n",
    "    print(\"✓ Repo already exists\")\n",
    "    \n",
    "%cd semantic_id_recommender\n",
    "!git pull --ff-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (let uv resolve versions compatible with Colab's environment)\n",
    "!uv pip install --system \\\n",
    "    torch torchvision torchaudio \\\n",
    "    transformers datasets accelerate \\\n",
    "    vector-quantize-pytorch sentence-transformers \\\n",
    "    lightning wandb omegaconf einops tqdm rich \\\n",
    "    pydantic\n",
    "\n",
    "# Fix protobuf version conflict\n",
    "!uv pip install --system \"protobuf>=3.20,<5\"\n",
    "\n",
    "# Note: We skip using uv.lock because Colab has pre-installed packages\n",
    "# that may conflict with locked versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import from Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add repo root to Python path for imports\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "# Import from the cloned package\n",
    "import torch\n",
    "from src.rqvae.model import SemanticRQVAE, SemanticRQVAEConfig\n",
    "from src.rqvae.trainer import RQVAETrainer\n",
    "from src.rqvae.dataset import ItemEmbeddingDataset\n",
    "\n",
    "print(\"✓ Package imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "items = []\n",
    "with open(\"data/ag_news_500.jsonl\") as f:\n",
    "    for line in f:\n",
    "        items.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(items)} items\")\n",
    "print(f\"\\nKeys: {items[0].keys()}\")\n",
    "print(f\"\\nSample items:\")\n",
    "for item in items[:5]:\n",
    "    print(f\"  [{item['category']}] {item['item_id']}: {item['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the package's ItemEmbeddingDataset to generate embeddings\n",
    "dataset = ItemEmbeddingDataset.from_catalogue(\n",
    "    catalogue_path=\"data/ag_news_500.jsonl\",\n",
    "    cache_path=\"data/embeddings.pt\",  # Cache for faster reruns\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Number of items: {len(dataset)}\")\n",
    "print(f\"  Embedding dim: {dataset.embeddings.shape[1]}\")\n",
    "print(f\"  Item IDs (first 5): {dataset.item_ids[:5]}\")\n",
    "\n",
    "# Keep embeddings tensor for later use\n",
    "embeddings = dataset.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Category distribution\n",
    "categories = [item['category'] for item in items]\n",
    "cat_counts = Counter(categories)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart of categories\n",
    "ax1 = axes[0]\n",
    "ax1.bar(cat_counts.keys(), cat_counts.values(), color=['#2ecc71', '#3498db', '#e74c3c', '#9b59b6'])\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Distribution by Category')\n",
    "for i, (cat, count) in enumerate(cat_counts.items()):\n",
    "    ax1.text(i, count + 2, str(count), ha='center')\n",
    "\n",
    "# Content length distribution\n",
    "content_lengths = [len(item['content']) for item in items]\n",
    "ax2 = axes[1]\n",
    "ax2.hist(content_lengths, bins=30, color='#3498db', edgecolor='white')\n",
    "ax2.set_xlabel('Content Length (chars)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distribution of Article Lengths')\n",
    "ax2.axvline(sum(content_lengths)/len(content_lengths), color='red', linestyle='--', label=f'Mean: {sum(content_lengths)//len(content_lengths)}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total articles: {len(items)}\")\n",
    "print(f\"  Categories: {dict(cat_counts)}\")\n",
    "print(f\"  Avg content length: {sum(content_lengths)//len(content_lengths)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train RQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "config = SemanticRQVAEConfig(\n",
    "    embedding_dim=384,\n",
    "    hidden_dim=128,\n",
    "    codebook_size=16,\n",
    "    num_quantizers=4,\n",
    "    threshold_ema_dead_code=1,\n",
    ")\n",
    "trainer_module = RQVAETrainer(config=config, learning_rate=1e-3)\n",
    "\n",
    "# Split dataset into train/val (90/10)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "trainer.fit(trainer_module, train_loader, val_loader)\n",
    "print(\"✓ RQ-VAE training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training diagnostics\n",
    "print(\"=== Training Diagnostics ===\\n\")\n",
    "\n",
    "# Get logged metrics from trainer\n",
    "logged_metrics = trainer.logged_metrics\n",
    "print(\"Final logged metrics:\")\n",
    "for key, value in sorted(logged_metrics.items()):\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Compute codebook stats on full dataset\n",
    "model = trainer_module.model\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_indices = model.get_semantic_ids(embeddings.to(device))\n",
    "    stats = model.compute_codebook_stats(all_indices)\n",
    "\n",
    "print(f\"\\n=== Codebook Statistics (full dataset) ===\")\n",
    "print(f\"Average perplexity: {stats['avg_perplexity']:.2f} / {config.codebook_size} (max)\")\n",
    "print(f\"Average usage: {stats['avg_usage']*100:.1f}%\")\n",
    "print(f\"\\nPer-level breakdown:\")\n",
    "for q in range(config.num_quantizers):\n",
    "    perp = stats['perplexity_per_level'][q].item()\n",
    "    usage = stats['usage_per_level'][q].item() * 100\n",
    "    print(f\"  Level {q}: perplexity={perp:.1f}, usage={usage:.1f}%\")\n",
    "\n",
    "# Check for semantic ID collisions\n",
    "unique_ids = len(set(model.semantic_id_to_string(all_indices)))\n",
    "print(f\"\\n=== Semantic ID Quality ===\")\n",
    "print(f\"Total items: {len(embeddings)}\")\n",
    "print(f\"Unique semantic IDs: {unique_ids}\")\n",
    "print(f\"Collision rate: {(1 - unique_ids/len(embeddings))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate semantic IDs for all items using the trained model\n",
    "model = trainer_module.model\n",
    "model.eval()\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "with torch.no_grad():\n",
    "    indices = model.get_semantic_ids(embeddings.to(device))\n",
    "    semantic_strings = model.semantic_id_to_string(indices)\n",
    "\n",
    "# Create mapping\n",
    "item_to_semantic = {}\n",
    "semantic_to_item = {}\n",
    "\n",
    "for i, item in enumerate(items):\n",
    "    item_id = item[\"item_id\"]  # AG News uses item_id\n",
    "    sem_id = semantic_strings[i]\n",
    "    item_to_semantic[item_id] = {\n",
    "        \"codes\": indices[i].cpu().tolist(),\n",
    "        \"semantic_id\": sem_id,\n",
    "    }\n",
    "    semantic_to_item[sem_id] = item_id\n",
    "\n",
    "# Save mapping\n",
    "mapping = {\n",
    "    \"item_to_semantic\": item_to_semantic,\n",
    "    \"semantic_to_item\": semantic_to_item,\n",
    "    \"config\": {\"num_quantizers\": config.num_quantizers, \"codebook_size\": config.codebook_size}\n",
    "}\n",
    "\n",
    "with open(\"data/semantic_ids.json\", \"w\") as f:\n",
    "    json.dump(mapping, f, indent=2)\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample semantic IDs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {items[i]['title'][:30]:30} -> {semantic_strings[i]}\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(semantic_to_item)} unique semantic IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine embedding similarity vs semantic ID similarity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Compute pairwise cosine similarities\n",
    "normed = F.normalize(embeddings, dim=1)\n",
    "similarities = normed @ normed.T\n",
    "\n",
    "# Mask out diagonal and near-duplicates (sim > 0.99)\n",
    "similarities_filtered = similarities.clone()\n",
    "similarities_filtered.fill_diagonal_(-1)\n",
    "mask = similarities_filtered > 0.99\n",
    "similarities_filtered[mask] = -1  # Exclude near-duplicates\n",
    "\n",
    "# Find most similar pair (excluding duplicates)\n",
    "max_idx = similarities_filtered.argmax()\n",
    "i_close, j_close = max_idx // len(items), max_idx % len(items)\n",
    "\n",
    "# Find most dissimilar pair\n",
    "similarities_for_min = similarities.clone()\n",
    "similarities_for_min.fill_diagonal_(2)  # Exclude self\n",
    "min_idx = similarities_for_min.argmin()\n",
    "i_far, j_far = min_idx // len(items), min_idx % len(items)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLOSEST PAIR (high embedding similarity, excluding duplicates)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Cosine similarity: {similarities[i_close, j_close]:.4f}\")\n",
    "print(f\"\\nItem A [{items[i_close]['category']}]:\")\n",
    "print(f\"  Title: {items[i_close]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[i_close]}\")\n",
    "print(f\"\\nItem B [{items[j_close]['category']}]:\")\n",
    "print(f\"  Title: {items[j_close]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[j_close]}\")\n",
    "\n",
    "# Check how many codes match\n",
    "codes_a = indices[i_close].tolist()\n",
    "codes_b = indices[j_close].tolist()\n",
    "matching = sum(a == b for a, b in zip(codes_a, codes_b))\n",
    "print(f\"\\nCodes A: {codes_a}\")\n",
    "print(f\"Codes B: {codes_b}\")\n",
    "print(f\"Matching codes: {matching}/{len(codes_a)} ✓\" if matching > 0 else f\"Matching codes: {matching}/{len(codes_a)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FARTHEST PAIR (low embedding similarity)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Cosine similarity: {similarities[i_far, j_far]:.4f}\")\n",
    "print(f\"\\nItem A [{items[i_far]['category']}]:\")\n",
    "print(f\"  Title: {items[i_far]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[i_far]}\")\n",
    "print(f\"\\nItem B [{items[j_far]['category']}]:\")\n",
    "print(f\"  Title: {items[j_far]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[j_far]}\")\n",
    "\n",
    "codes_a = indices[i_far].tolist()\n",
    "codes_b = indices[j_far].tolist()\n",
    "matching = sum(a == b for a, b in zip(codes_a, codes_b))\n",
    "print(f\"\\nCodes A: {codes_a}\")\n",
    "print(f\"Codes B: {codes_b}\")\n",
    "print(f\"Matching codes: {matching}/{len(codes_a)}\")\n",
    "\n",
    "# Check for duplicates in dataset\n",
    "dup_count = (similarities > 0.999).sum().item() - len(items)  # Subtract diagonal\n",
    "print(f\"\\n⚠️  Found {dup_count//2} duplicate pairs in dataset (cosine sim > 0.999)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate query -> semantic ID training pairs\n",
    "query_templates = [\n",
    "    \"Find me a {title}\",\n",
    "    \"I'm looking for {title}\",\n",
    "    \"Recommend a {title}\",\n",
    "    \"{title}\",\n",
    "    \"{description}\",\n",
    "]\n",
    "\n",
    "training_examples = []\n",
    "system_prompt = \"You are a recommendation system. Given a user query, output the semantic ID of the most relevant item.\"\n",
    "\n",
    "for item in items:\n",
    "    item_id = item[\"id\"]\n",
    "    sem_id = item_to_semantic[item_id][\"semantic_id\"]\n",
    "    \n",
    "    for template in random.sample(query_templates, 2):\n",
    "        query = template.format(title=item[\"title\"], description=item[\"description\"])\n",
    "        \n",
    "        training_examples.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "                {\"role\": \"assistant\", \"content\": sem_id},\n",
    "            ],\n",
    "            \"item_id\": item_id,\n",
    "        })\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(training_examples)\n",
    "split_idx = int(len(training_examples) * 0.9)\n",
    "train_examples = training_examples[:split_idx]\n",
    "val_examples = training_examples[split_idx:]\n",
    "\n",
    "# Save\n",
    "with open(\"data/train.jsonl\", \"w\") as f:\n",
    "    for ex in train_examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "with open(\"data/val.jsonl\", \"w\") as f:\n",
    "    for ex in val_examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Generated {len(train_examples)} train, {len(val_examples)} val examples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Query: {train_examples[0]['messages'][1]['content'][:50]}...\")\n",
    "print(f\"  Target: {train_examples[0]['messages'][2]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tune LLM with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "# Disable wandb for this test\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Use a small model for testing\n",
    "BASE_MODEL = \"unsloth/Qwen2.5-0.5B\"  # Very small for quick test\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "print(f\"Loading {BASE_MODEL}...\")\n",
    "llm_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add semantic ID tokens\n",
    "semantic_tokens = []\n",
    "for q in range(config.num_quantizers):\n",
    "    for c in range(config.codebook_size):\n",
    "        semantic_tokens.append(f\"[SEM_{q}_{c}]\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": semantic_tokens})\n",
    "llm_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"✓ Added {len(semantic_tokens)} semantic tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "llm_model = FastLanguageModel.get_peft_model(\n",
    "    llm_model,\n",
    "    r=8,  # Small for quick test\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"✓ LoRA adapters added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def formatting_func(examples):\n",
    "    output_texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        text = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            if role == \"system\":\n",
    "                text += f\"<|system|>\\n{content}\\n\"\n",
    "            elif role == \"user\":\n",
    "                text += f\"<|user|>\\n{content}\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                text += f\"<|assistant|>\\n{content}\\n\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "val_dataset = Dataset.from_list(val_examples)\n",
    "\n",
    "print(f\"✓ Dataset prepared: {len(train_dataset)} train, {len(val_dataset)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (quick test settings)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/llm\",\n",
    "    num_train_epochs=1,  # Just 1 epoch for test\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",  # Don't save for test\n",
    "    eval_strategy=\"no\",\n",
    "    fp16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to inference mode\n",
    "FastLanguageModel.for_inference(llm_model)\n",
    "\n",
    "def generate_recommendation(query: str) -> str:\n",
    "    system_prompt = \"You are a recommendation system. Given a user query, output the semantic ID of the most relevant item.\"\n",
    "    prompt = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{query}\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm_model.device)\n",
    "    \n",
    "    outputs = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=32,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # Extract part after <|assistant|>\n",
    "    if \"<|assistant|>\" in generated:\n",
    "        result = generated.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        result = generated[len(prompt):].strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Inference mode ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Find me a premium electronics widget\",\n",
    "    \"I need a budget sports accessory\",\n",
    "    \"Recommend a luxury home device\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in test_queries:\n",
    "    result = generate_recommendation(query)\n",
    "    \n",
    "    # Try to find the item\n",
    "    import re\n",
    "    sem_id_match = re.search(r\"(\\[SEM_\\d+_\\d+\\])+\", result)\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Generated: {result[:100]}\")\n",
    "    \n",
    "    if sem_id_match:\n",
    "        sem_id = sem_id_match.group(0)\n",
    "        if sem_id in semantic_to_item:\n",
    "            item_id = semantic_to_item[sem_id]\n",
    "            item = next(i for i in items if i[\"id\"] == item_id)\n",
    "            print(f\"Matched: {item['title']}\")\n",
    "        else:\n",
    "            print(f\"Semantic ID not in catalogue: {sem_id}\")\n",
    "    else:\n",
    "        print(\"No valid semantic ID found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ All tests complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested:\n",
    "\n",
    "1. **RQ-VAE Training** - ✓ Model learns to encode items as discrete codes\n",
    "2. **Semantic ID Generation** - ✓ Each item gets a unique semantic ID\n",
    "3. **Training Data Creation** - ✓ Query → Semantic ID pairs generated\n",
    "4. **LLM Fine-tuning** - ✓ Model learns to predict semantic IDs\n",
    "5. **Inference** - ✓ Model generates semantic IDs for new queries\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "For production use:\n",
    "- Use larger model (Qwen3-4B or Ministral-3B)\n",
    "- Train for more epochs (3-5)\n",
    "- Use full catalogue\n",
    "- Deploy to Modal for serverless inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
