{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic ID Recommender - Training Test\n",
    "\n",
    "This notebook tests the full training pipeline in Google Colab.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 is sufficient)\n",
    "- ~10 minutes for full test\n",
    "\n",
    "**What this tests:**\n",
    "1. RQ-VAE training (semantic ID learning)\n",
    "2. Training data generation\n",
    "3. LLM fine-tuning with Unsloth\n",
    "4. Inference test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers datasets accelerate\n",
    "!pip install -q vector-quantize-pytorch sentence-transformers\n",
    "!pip install -q lightning wandb omegaconf einops tqdm rich\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project structure\n",
    "!mkdir -p src/config src/rqvae src/llm data checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Models & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ-VAE Model\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vector_quantize_pytorch import ResidualVQ\n",
    "\n",
    "@dataclass\n",
    "class SemanticRQVAEConfig:\n",
    "    embedding_dim: int = 384\n",
    "    hidden_dim: int = 512\n",
    "    codebook_size: int = 256\n",
    "    num_quantizers: int = 4\n",
    "    commitment_weight: float = 0.25\n",
    "    decay: float = 0.99\n",
    "\n",
    "class SemanticRQVAE(nn.Module):\n",
    "    def __init__(self, config: SemanticRQVAEConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(config.embedding_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.rq = ResidualVQ(\n",
    "            dim=config.hidden_dim,\n",
    "            codebook_size=config.codebook_size,\n",
    "            num_quantizers=config.num_quantizers,\n",
    "            commitment_weight=config.commitment_weight,\n",
    "            decay=config.decay,\n",
    "            kmeans_init=True,\n",
    "            threshold_ema_dead_code=2,\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_dim, config.embedding_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_unsqueezed = z.unsqueeze(1)\n",
    "        quantized, indices, commit_loss = self.rq(z_unsqueezed)\n",
    "        quantized = quantized.squeeze(1)\n",
    "        indices = indices.squeeze(1)\n",
    "        if commit_loss.dim() > 0:\n",
    "            commit_loss = commit_loss.sum()\n",
    "        reconstructed = self.decoder(quantized)\n",
    "        recon_loss = nn.functional.mse_loss(reconstructed, x)\n",
    "        return reconstructed, indices, recon_loss, commit_loss\n",
    "    \n",
    "    def get_semantic_ids(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_unsqueezed = z.unsqueeze(1)\n",
    "        _, indices, _ = self.rq(z_unsqueezed)\n",
    "        return indices.squeeze(1)\n",
    "    \n",
    "    def semantic_id_to_string(self, indices):\n",
    "        batch_size = indices.shape[0]\n",
    "        results = []\n",
    "        for i in range(batch_size):\n",
    "            tokens = [f\"[SEM_{q}_{indices[i, q].item()}]\" for q in range(self.config.num_quantizers)]\n",
    "            results.append(\"\".join(tokens))\n",
    "        return results\n",
    "\n",
    "print(\"✓ RQ-VAE model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Generate dummy catalogue\n",
    "categories = [\"Electronics\", \"Books\", \"Clothing\", \"Home\", \"Sports\"]\n",
    "adjectives = [\"Premium\", \"Budget\", \"Professional\", \"Compact\", \"Luxury\"]\n",
    "nouns = [\"Widget\", \"Gadget\", \"Device\", \"Tool\", \"Accessory\"]\n",
    "\n",
    "NUM_ITEMS = 500  # Small for quick testing\n",
    "\n",
    "items = []\n",
    "for i in range(NUM_ITEMS):\n",
    "    category = random.choice(categories)\n",
    "    adj = random.choice(adjectives)\n",
    "    noun = random.choice(nouns)\n",
    "    items.append({\n",
    "        \"id\": f\"item_{i:05d}\",\n",
    "        \"title\": f\"{adj} {category} {noun}\",\n",
    "        \"description\": f\"A high-quality {adj.lower()} {noun.lower()} for {category.lower()} enthusiasts.\",\n",
    "        \"category\": category,\n",
    "    })\n",
    "\n",
    "with open(\"data/catalogue.json\", \"w\") as f:\n",
    "    json.dump({\"items\": items}, f)\n",
    "\n",
    "print(f\"✓ Created catalogue with {NUM_ITEMS} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate embeddings\n",
    "texts = [f\"{item['title']}. {item['description']}\" for item in items]\n",
    "embeddings = embed_model.encode(texts, show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "print(f\"✓ Generated embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train RQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config for quick test\n",
    "config = SemanticRQVAEConfig(\n",
    "    embedding_dim=384,\n",
    "    hidden_dim=256,\n",
    "    codebook_size=64,  # Smaller for quick test\n",
    "    num_quantizers=4,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SemanticRQVAE(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(embeddings.to(device))\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train\n",
    "NUM_EPOCHS = 20\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        _, _, recon_loss, commit_loss = model(x)\n",
    "        loss = recon_loss + commit_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"✓ RQ-VAE training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate semantic IDs for all items\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    indices = model.get_semantic_ids(embeddings.to(device))\n",
    "    semantic_strings = model.semantic_id_to_string(indices)\n",
    "\n",
    "# Create mapping\n",
    "item_to_semantic = {}\n",
    "semantic_to_item = {}\n",
    "\n",
    "for i, item in enumerate(items):\n",
    "    item_id = item[\"id\"]\n",
    "    sem_id = semantic_strings[i]\n",
    "    item_to_semantic[item_id] = {\n",
    "        \"codes\": indices[i].cpu().tolist(),\n",
    "        \"semantic_id\": sem_id,\n",
    "    }\n",
    "    semantic_to_item[sem_id] = item_id\n",
    "\n",
    "# Save mapping\n",
    "mapping = {\n",
    "    \"item_to_semantic\": item_to_semantic,\n",
    "    \"semantic_to_item\": semantic_to_item,\n",
    "    \"config\": {\"num_quantizers\": config.num_quantizers, \"codebook_size\": config.codebook_size}\n",
    "}\n",
    "\n",
    "with open(\"data/semantic_ids.json\", \"w\") as f:\n",
    "    json.dump(mapping, f, indent=2)\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample semantic IDs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {items[i]['title'][:30]:30} -> {semantic_strings[i]}\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(semantic_to_item)} unique semantic IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate query -> semantic ID training pairs\n",
    "query_templates = [\n",
    "    \"Find me a {title}\",\n",
    "    \"I'm looking for {title}\",\n",
    "    \"Recommend a {title}\",\n",
    "    \"{title}\",\n",
    "    \"{description}\",\n",
    "]\n",
    "\n",
    "training_examples = []\n",
    "system_prompt = \"You are a recommendation system. Given a user query, output the semantic ID of the most relevant item.\"\n",
    "\n",
    "for item in items:\n",
    "    item_id = item[\"id\"]\n",
    "    sem_id = item_to_semantic[item_id][\"semantic_id\"]\n",
    "    \n",
    "    for template in random.sample(query_templates, 2):\n",
    "        query = template.format(title=item[\"title\"], description=item[\"description\"])\n",
    "        \n",
    "        training_examples.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "                {\"role\": \"assistant\", \"content\": sem_id},\n",
    "            ],\n",
    "            \"item_id\": item_id,\n",
    "        })\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(training_examples)\n",
    "split_idx = int(len(training_examples) * 0.9)\n",
    "train_examples = training_examples[:split_idx]\n",
    "val_examples = training_examples[split_idx:]\n",
    "\n",
    "# Save\n",
    "with open(\"data/train.jsonl\", \"w\") as f:\n",
    "    for ex in train_examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "with open(\"data/val.jsonl\", \"w\") as f:\n",
    "    for ex in val_examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Generated {len(train_examples)} train, {len(val_examples)} val examples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Query: {train_examples[0]['messages'][1]['content'][:50]}...\")\n",
    "print(f\"  Target: {train_examples[0]['messages'][2]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tune LLM with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "# Disable wandb for this test\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Use a small model for testing\n",
    "BASE_MODEL = \"unsloth/Qwen2.5-0.5B\"  # Very small for quick test\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "print(f\"Loading {BASE_MODEL}...\")\n",
    "llm_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add semantic ID tokens\n",
    "semantic_tokens = []\n",
    "for q in range(config.num_quantizers):\n",
    "    for c in range(config.codebook_size):\n",
    "        semantic_tokens.append(f\"[SEM_{q}_{c}]\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": semantic_tokens})\n",
    "llm_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"✓ Added {len(semantic_tokens)} semantic tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "llm_model = FastLanguageModel.get_peft_model(\n",
    "    llm_model,\n",
    "    r=8,  # Small for quick test\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"✓ LoRA adapters added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def formatting_func(examples):\n",
    "    output_texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        text = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            if role == \"system\":\n",
    "                text += f\"<|system|>\\n{content}\\n\"\n",
    "            elif role == \"user\":\n",
    "                text += f\"<|user|>\\n{content}\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                text += f\"<|assistant|>\\n{content}\\n\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "val_dataset = Dataset.from_list(val_examples)\n",
    "\n",
    "print(f\"✓ Dataset prepared: {len(train_dataset)} train, {len(val_dataset)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (quick test settings)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/llm\",\n",
    "    num_train_epochs=1,  # Just 1 epoch for test\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",  # Don't save for test\n",
    "    eval_strategy=\"no\",\n",
    "    fp16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to inference mode\n",
    "FastLanguageModel.for_inference(llm_model)\n",
    "\n",
    "def generate_recommendation(query: str) -> str:\n",
    "    system_prompt = \"You are a recommendation system. Given a user query, output the semantic ID of the most relevant item.\"\n",
    "    prompt = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{query}\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm_model.device)\n",
    "    \n",
    "    outputs = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=32,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # Extract part after <|assistant|>\n",
    "    if \"<|assistant|>\" in generated:\n",
    "        result = generated.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        result = generated[len(prompt):].strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Inference mode ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Find me a premium electronics widget\",\n",
    "    \"I need a budget sports accessory\",\n",
    "    \"Recommend a luxury home device\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in test_queries:\n",
    "    result = generate_recommendation(query)\n",
    "    \n",
    "    # Try to find the item\n",
    "    import re\n",
    "    sem_id_match = re.search(r\"(\\[SEM_\\d+_\\d+\\])+\", result)\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Generated: {result[:100]}\")\n",
    "    \n",
    "    if sem_id_match:\n",
    "        sem_id = sem_id_match.group(0)\n",
    "        if sem_id in semantic_to_item:\n",
    "            item_id = semantic_to_item[sem_id]\n",
    "            item = next(i for i in items if i[\"id\"] == item_id)\n",
    "            print(f\"Matched: {item['title']}\")\n",
    "        else:\n",
    "            print(f\"Semantic ID not in catalogue: {sem_id}\")\n",
    "    else:\n",
    "        print(\"No valid semantic ID found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ All tests complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested:\n",
    "\n",
    "1. **RQ-VAE Training** - ✓ Model learns to encode items as discrete codes\n",
    "2. **Semantic ID Generation** - ✓ Each item gets a unique semantic ID\n",
    "3. **Training Data Creation** - ✓ Query → Semantic ID pairs generated\n",
    "4. **LLM Fine-tuning** - ✓ Model learns to predict semantic IDs\n",
    "5. **Inference** - ✓ Model generates semantic IDs for new queries\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "For production use:\n",
    "- Use larger model (Qwen3-4B or Ministral-3B)\n",
    "- Train for more epochs (3-5)\n",
    "- Use full catalogue\n",
    "- Deploy to Modal for serverless inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
