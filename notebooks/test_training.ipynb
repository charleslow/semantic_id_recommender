{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic ID Recommender - Training Test\n",
    "\n",
    "This notebook tests the full training pipeline in Google Colab.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 is sufficient)\n",
    "- ~10 minutes for full test\n",
    "\n",
    "**What this tests:**\n",
    "1. RQ-VAE training (semantic ID learning)\n",
    "2. Training data generation\n",
    "3. LLM fine-tuning with Unsloth\n",
    "4. Inference test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv\n",
    "!pip install uv\n",
    "\n",
    "# Clone private repo (skip if already exists)\n",
    "import os\n",
    "if not os.path.exists(\"semantic_id_recommender\"):\n",
    "    !git clone https://github.com/charleslow/semantic_id_recommender.git\n",
    "else:\n",
    "    print(\"✓ Repo already exists\")\n",
    "    \n",
    "%cd semantic_id_recommender\n",
    "!git pull --ff-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (let uv resolve versions compatible with Colab's environment)\n",
    "!uv pip install --system \\\n",
    "    torch torchvision torchaudio \\\n",
    "    transformers datasets accelerate \\\n",
    "    vector-quantize-pytorch sentence-transformers \\\n",
    "    lightning wandb omegaconf einops tqdm rich \\\n",
    "    pydantic unsloth psutil\n",
    "\n",
    "# Fix protobuf version conflict\n",
    "!uv pip install --system \"protobuf>=3.20,<5\"\n",
    "\n",
    "# Note: We skip using uv.lock because Colab has pre-installed packages\n",
    "# that may conflict with locked versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import from Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add repo root to Python path for imports\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "# Import from the cloned package\n",
    "import torch\n",
    "from src.rqvae.model import SemanticRQVAE, SemanticRQVAEConfig\n",
    "from src.rqvae.trainer import RQVAETrainer\n",
    "from src.rqvae.dataset import ItemEmbeddingDataset\n",
    "\n",
    "print(\"✓ Package imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "items = []\n",
    "with open(\"data/ag_news_500.jsonl\") as f:\n",
    "    for line in f:\n",
    "        items.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(items)} items\")\n",
    "print(f\"\\nKeys: {items[0].keys()}\")\n",
    "print(f\"\\nSample items:\")\n",
    "for item in items[:5]:\n",
    "    print(f\"  [{item['category']}] {item['item_id']}: {item['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the package's ItemEmbeddingDataset to generate embeddings\n",
    "# Using GTE-Qwen2 - a powerful Qwen-based embedding model (1536 dim)\n",
    "dataset = ItemEmbeddingDataset.from_catalogue(\n",
    "    catalogue_path=\"data/ag_news_500.jsonl\",\n",
    "    embedding_model=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    cache_path=\"data/embeddings_qwen3_0.6b.pt\",  # New cache for different model\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Number of items: {len(dataset)}\")\n",
    "print(f\"  Embedding dim: {dataset.embeddings.shape[1]}\")\n",
    "print(f\"  Item IDs (first 5): {dataset.item_ids[:5]}\")\n",
    "\n",
    "# Keep embeddings tensor for later use\n",
    "embeddings = dataset.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Category distribution\n",
    "categories = [item['category'] for item in items]\n",
    "cat_counts = Counter(categories)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart of categories\n",
    "ax1 = axes[0]\n",
    "ax1.bar(cat_counts.keys(), cat_counts.values(), color=['#2ecc71', '#3498db', '#e74c3c', '#9b59b6'])\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Distribution by Category')\n",
    "for i, (cat, count) in enumerate(cat_counts.items()):\n",
    "    ax1.text(i, count + 2, str(count), ha='center')\n",
    "\n",
    "# Content length distribution\n",
    "content_lengths = [len(item['content']) for item in items]\n",
    "ax2 = axes[1]\n",
    "ax2.hist(content_lengths, bins=30, color='#3498db', edgecolor='white')\n",
    "ax2.set_xlabel('Content Length (chars)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distribution of Article Lengths')\n",
    "ax2.axvline(sum(content_lengths)/len(content_lengths), color='red', linestyle='--', label=f'Mean: {sum(content_lengths)//len(content_lengths)}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total articles: {len(items)}\")\n",
    "print(f\"  Categories: {dict(cat_counts)}\")\n",
    "print(f\"  Avg content length: {sum(content_lengths)//len(content_lengths)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train RQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb for the entire experiment\n",
    "wandb.init(\n",
    "    project=\"semantic-id-recommender\",\n",
    "    name=\"full-pipeline-test\",\n",
    "    config={\n",
    "        \"embedding_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"num_items\": len(items),\n",
    "    }\n",
    ")\n",
    "\n",
    "config = SemanticRQVAEConfig(\n",
    "    embedding_dim=1024,  # Match embedding model output dimension\n",
    "    hidden_dim=256,      # Larger hidden dim for bigger embeddings\n",
    "    codebook_size=32,\n",
    "    num_quantizers=3,\n",
    "    threshold_ema_dead_code=1,\n",
    ")\n",
    "\n",
    "# Log RQ-VAE config to wandb\n",
    "wandb.config.update({\n",
    "    \"rqvae_embedding_dim\": config.embedding_dim,\n",
    "    \"rqvae_hidden_dim\": config.hidden_dim,\n",
    "    \"rqvae_codebook_size\": config.codebook_size,\n",
    "    \"rqvae_num_quantizers\": config.num_quantizers,\n",
    "})\n",
    "\n",
    "trainer_module = RQVAETrainer(config=config, learning_rate=1e-3)\n",
    "\n",
    "# Split dataset into train/val (90/10)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# Create WandbLogger for Lightning\n",
    "wandb_logger = WandbLogger(experiment=wandb.run)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1,\n",
    "    logger=wandb_logger,  # Log to wandb\n",
    ")\n",
    "trainer.fit(trainer_module, train_loader, val_loader)\n",
    "print(\"✓ RQ-VAE training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training diagnostics\n",
    "print(\"=== Training Diagnostics ===\\n\")\n",
    "\n",
    "# Get logged metrics from trainer\n",
    "logged_metrics = trainer.logged_metrics\n",
    "print(\"Final logged metrics:\")\n",
    "for key, value in sorted(logged_metrics.items()):\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Compute codebook stats on full dataset\n",
    "model = trainer_module.model\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_indices = model.get_semantic_ids(embeddings.to(device))\n",
    "    stats = model.compute_codebook_stats(all_indices)\n",
    "\n",
    "print(f\"\\n=== Codebook Statistics (full dataset) ===\")\n",
    "print(f\"Average perplexity: {stats['avg_perplexity']:.2f} / {config.codebook_size} (max)\")\n",
    "print(f\"Average usage: {stats['avg_usage']*100:.1f}%\")\n",
    "print(f\"\\nPer-level breakdown:\")\n",
    "for q in range(config.num_quantizers):\n",
    "    perp = stats['perplexity_per_level'][q].item()\n",
    "    usage = stats['usage_per_level'][q].item() * 100\n",
    "    print(f\"  Level {q}: perplexity={perp:.1f}, usage={usage:.1f}%\")\n",
    "\n",
    "# Check for semantic ID collisions\n",
    "unique_ids = len(set(model.semantic_id_to_string(all_indices)))\n",
    "print(f\"\\n=== Semantic ID Quality ===\")\n",
    "print(f\"Total items: {len(embeddings)}\")\n",
    "print(f\"Unique semantic IDs: {unique_ids}\")\n",
    "print(f\"Collision rate: {(1 - unique_ids/len(embeddings))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate semantic IDs for all items using the trained model\n",
    "model = trainer_module.model\n",
    "model.eval()\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "with torch.no_grad():\n",
    "    indices = model.get_semantic_ids(embeddings.to(device))\n",
    "    semantic_strings = model.semantic_id_to_string(indices)\n",
    "\n",
    "# Create mapping\n",
    "item_to_semantic = {}\n",
    "semantic_to_item = {}\n",
    "\n",
    "for i, item in enumerate(items):\n",
    "    item_id = item[\"item_id\"]  # AG News uses item_id\n",
    "    sem_id = semantic_strings[i]\n",
    "    item_to_semantic[item_id] = {\n",
    "        \"codes\": indices[i].cpu().tolist(),\n",
    "        \"semantic_id\": sem_id,\n",
    "    }\n",
    "    semantic_to_item[sem_id] = item_id\n",
    "\n",
    "# Save mapping\n",
    "mapping = {\n",
    "    \"item_to_semantic\": item_to_semantic,\n",
    "    \"semantic_to_item\": semantic_to_item,\n",
    "    \"config\": {\"num_quantizers\": config.num_quantizers, \"codebook_size\": config.codebook_size}\n",
    "}\n",
    "\n",
    "with open(\"data/semantic_ids.json\", \"w\") as f:\n",
    "    json.dump(mapping, f, indent=2)\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample semantic IDs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {items[i]['title'][:30]:30} -> {semantic_strings[i]}\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(semantic_to_item)} unique semantic IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine embedding similarity vs semantic ID similarity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Compute pairwise cosine similarities\n",
    "normed = F.normalize(embeddings, dim=1)\n",
    "similarities = normed @ normed.T\n",
    "\n",
    "# Mask out diagonal and near-duplicates (sim > 0.99)\n",
    "similarities_filtered = similarities.clone()\n",
    "similarities_filtered.fill_diagonal_(-1)\n",
    "mask = similarities_filtered > 0.99\n",
    "similarities_filtered[mask] = -1  # Exclude near-duplicates\n",
    "\n",
    "# Find most similar pair (excluding duplicates)\n",
    "max_idx = similarities_filtered.argmax()\n",
    "i_close, j_close = max_idx // len(items), max_idx % len(items)\n",
    "\n",
    "# Find most dissimilar pair\n",
    "similarities_for_min = similarities.clone()\n",
    "similarities_for_min.fill_diagonal_(2)  # Exclude self\n",
    "min_idx = similarities_for_min.argmin()\n",
    "i_far, j_far = min_idx // len(items), min_idx % len(items)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLOSEST PAIR (high embedding similarity, excluding duplicates)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Cosine similarity: {similarities[i_close, j_close]:.4f}\")\n",
    "print(f\"\\nItem A [{items[i_close]['category']}]:\")\n",
    "print(f\"  Title: {items[i_close]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[i_close]}\")\n",
    "print(f\"\\nItem B [{items[j_close]['category']}]:\")\n",
    "print(f\"  Title: {items[j_close]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[j_close]}\")\n",
    "\n",
    "# Check how many codes match\n",
    "codes_a = indices[i_close].tolist()\n",
    "codes_b = indices[j_close].tolist()\n",
    "matching = sum(a == b for a, b in zip(codes_a, codes_b))\n",
    "print(f\"\\nCodes A: {codes_a}\")\n",
    "print(f\"Codes B: {codes_b}\")\n",
    "print(f\"Matching codes: {matching}/{len(codes_a)} ✓\" if matching > 0 else f\"Matching codes: {matching}/{len(codes_a)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FARTHEST PAIR (low embedding similarity)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Cosine similarity: {similarities[i_far, j_far]:.4f}\")\n",
    "print(f\"\\nItem A [{items[i_far]['category']}]:\")\n",
    "print(f\"  Title: {items[i_far]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[i_far]}\")\n",
    "print(f\"\\nItem B [{items[j_far]['category']}]:\")\n",
    "print(f\"  Title: {items[j_far]['title'][:80]}\")\n",
    "print(f\"  Semantic ID: {semantic_strings[j_far]}\")\n",
    "\n",
    "codes_a = indices[i_far].tolist()\n",
    "codes_b = indices[j_far].tolist()\n",
    "matching = sum(a == b for a, b in zip(codes_a, codes_b))\n",
    "print(f\"\\nCodes A: {codes_a}\")\n",
    "print(f\"Codes B: {codes_b}\")\n",
    "print(f\"Matching codes: {matching}/{len(codes_a)}\")\n",
    "\n",
    "# Check for duplicates in dataset\n",
    "dup_count = (similarities > 0.999).sum().item() - len(items)  # Subtract diagonal\n",
    "print(f\"\\n⚠️  Found {dup_count//2} duplicate pairs in dataset (cosine sim > 0.999)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data using the package utilities\n",
    "from src.llm.data import format_for_chat\n",
    "\n",
    "# Create items with semantic IDs for training\n",
    "items_with_sem_ids = []\n",
    "for item in items:\n",
    "    item_copy = item.copy()\n",
    "    item_copy[\"semantic_id\"] = item_to_semantic[item[\"item_id\"]][\"semantic_id\"]\n",
    "    items_with_sem_ids.append(item_copy)\n",
    "\n",
    "# Generate training examples\n",
    "from src.llm.data import generate_training_examples\n",
    "raw_examples = generate_training_examples(\n",
    "    items_with_sem_ids, \n",
    "    num_examples_per_item=10,\n",
    "    query_templates=[\n",
    "        # Direct title queries\n",
    "        \"{title}\",\n",
    "        \"Find me news about {title}\",\n",
    "        \"I'm looking for articles on {title}\",\n",
    "        \"Show me {title}\",\n",
    "        \"Search for {title}\",\n",
    "        # Category-based queries\n",
    "        \"News about {category}: {title}\",\n",
    "        \"{category} news: {title}\",\n",
    "        \"Latest {category} article about {title}\",\n",
    "        # Question formats\n",
    "        \"What's the news about {title}?\",\n",
    "        \"Any articles on {title}?\",\n",
    "        \"Do you have news about {title}?\",\n",
    "        # Recommendation style\n",
    "        \"Recommend articles about {title}\",\n",
    "        \"Suggest news on {title}\",\n",
    "        # Short/casual queries\n",
    "        \"news {title}\",\n",
    "        \"article {title}\",\n",
    "        \"{category} {title}\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Format for chat and split\n",
    "import random\n",
    "formatted_examples = format_for_chat(raw_examples)\n",
    "random.shuffle(formatted_examples)\n",
    "split_idx = int(len(formatted_examples) * 0.9)\n",
    "train_examples = formatted_examples[:split_idx]\n",
    "val_examples = formatted_examples[split_idx:]\n",
    "\n",
    "print(f\"✓ Generated {len(train_examples)} train, {len(val_examples)} val examples\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Query: {train_examples[0]['messages'][1]['content'][:50]}...\")\n",
    "print(f\"  Target: {train_examples[0]['messages'][2]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tune LLM with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update wandb config for LLM training (wandb already initialized)\n",
    "wandb.config.update({\n",
    "    \"base_model\": \"unsloth/SmolLM2-135M-Instruct\",\n",
    "    \"num_train_examples\": len(train_examples),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "\n",
    "# Use SmolLM - very small model (135M parameters)\n",
    "BASE_MODEL = \"unsloth/SmolLM2-135M-Instruct\"\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "print(f\"Loading {BASE_MODEL}...\")\n",
    "llm_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add semantic ID tokens using package utility\n",
    "from src.llm.finetune import add_semantic_tokens\n",
    "\n",
    "tokenizer = add_semantic_tokens(tokenizer, config.num_quantizers, config.codebook_size)\n",
    "llm_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"✓ Added {config.num_quantizers * config.codebook_size} semantic tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "llm_model = FastLanguageModel.get_peft_model(\n",
    "    llm_model,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"✓ LoRA adapters added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset using package formatting function\n",
    "def format_single_example(messages):\n",
    "    \"\"\"Format messages into prompt string.\"\"\"\n",
    "    text = \"\"\n",
    "    for msg in messages:\n",
    "        role, content = msg[\"role\"], msg[\"content\"]\n",
    "        if role == \"system\":\n",
    "            text += f\"<|system|>\\n{content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            text += f\"<|user|>\\n{content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            text += f\"<|assistant|>\\n{content}\\n\"\n",
    "    return text\n",
    "\n",
    "train_texts = [{\"text\": format_single_example(ex[\"messages\"])} for ex in train_examples]\n",
    "val_texts = [{\"text\": format_single_example(ex[\"messages\"])} for ex in val_examples]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_texts)\n",
    "val_dataset = Dataset.from_list(val_texts)\n",
    "\n",
    "print(f\"✓ Dataset prepared: {len(train_dataset)} train, {len(val_dataset)} val\")\n",
    "print(f\"Sample: {train_texts[0]['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: Unsloth's compiled cache needs psutil in builtins\n",
    "import psutil\n",
    "import builtins\n",
    "builtins.psutil = psutil\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/llm\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",      # Enable evaluation\n",
    "    eval_steps=50,              # Evaluate every 50 steps\n",
    "    fp16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,   # Add validation dataset\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LLM fine-tuning script using Unsloth.\n",
    "\n",
    "Fine-tunes a base model to generate semantic IDs from user queries.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from .data import get_semantic_id_tokens\n",
    "\n",
    "\n",
    "def add_semantic_tokens(tokenizer, num_quantizers: int = 4, codebook_size: int = 256):\n",
    "    \"\"\"\n",
    "    Add semantic ID special tokens to tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        num_quantizers: Number of RQ-VAE quantizers\n",
    "        codebook_size: Size of each codebook\n",
    "\n",
    "    Returns:\n",
    "        Updated tokenizer\n",
    "    \"\"\"\n",
    "    special_tokens = get_semantic_id_tokens(num_quantizers, codebook_size)\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format examples for training using chat template.\"\"\"\n",
    "    output_texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        text = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            if role == \"system\":\n",
    "                text += f\"<|system|>\\n{content}\\n\"\n",
    "            elif role == \"user\":\n",
    "                text += f\"<|user|>\\n{content}\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                text += f\"<|assistant|>\\n{content}\\n\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def finetune_model(\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset | None = None,\n",
    "    base_model: str = \"unsloth/Qwen3-4B\",\n",
    "    output_dir: str = \"checkpoints/llm\",\n",
    "    num_quantizers: int = 4,\n",
    "    codebook_size: int = 256,\n",
    "    max_seq_length: int = 512,\n",
    "    lora_r: int = 16,\n",
    "    lora_alpha: int = 32,\n",
    "    lora_dropout: float = 0.05,\n",
    "    learning_rate: float = 2e-4,\n",
    "    batch_size: int = 4,\n",
    "    gradient_accumulation_steps: int = 4,\n",
    "    max_epochs: int = 3,\n",
    "    warmup_ratio: float = 0.03,\n",
    "    push_to_hub: bool = False,\n",
    "    hub_repo: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune LLM using Unsloth for semantic ID generation.\n",
    "\n",
    "    Args:\n",
    "        train_dataset: Training dataset\n",
    "        val_dataset: Validation dataset (optional)\n",
    "        base_model: Base model name (Unsloth-compatible)\n",
    "        output_dir: Directory to save checkpoints\n",
    "        num_quantizers: Number of semantic ID quantizers\n",
    "        codebook_size: Size of each codebook\n",
    "        max_seq_length: Maximum sequence length\n",
    "        lora_r: LoRA rank\n",
    "        lora_alpha: LoRA alpha\n",
    "        lora_dropout: LoRA dropout\n",
    "        learning_rate: Learning rate\n",
    "        batch_size: Batch size per device\n",
    "        gradient_accumulation_steps: Gradient accumulation steps\n",
    "        max_epochs: Number of training epochs\n",
    "        warmup_ratio: Warmup ratio\n",
    "        push_to_hub: Whether to push to HuggingFace Hub\n",
    "        hub_repo: Hub repository name\n",
    "\n",
    "    Returns:\n",
    "        Trained model and tokenizer\n",
    "    \"\"\"\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    # Load model with Unsloth\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,  # Auto-detect\n",
    "        load_in_4bit=True,  # QLoRA\n",
    "    )\n",
    "\n",
    "    # Add semantic ID tokens\n",
    "    tokenizer = add_semantic_tokens(tokenizer, num_quantizers, codebook_size)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Add LoRA adapters\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Memory efficient\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=max_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\" if val_dataset else \"no\",\n",
    "        fp16=True,\n",
    "        optim=\"adamw_8bit\",\n",
    "        report_to=\"wandb\",\n",
    "        push_to_hub=push_to_hub,\n",
    "        hub_model_id=hub_repo,\n",
    "    )\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        args=training_args,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Save\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Push to hub if configured\n",
    "    if push_to_hub and hub_repo:\n",
    "        model.push_to_hub(hub_repo)\n",
    "        tokenizer.push_to_hub(hub_repo)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_finetuned_model(\n",
    "    model_path: str,\n",
    "    max_seq_length: int = 512,\n",
    "    load_in_4bit: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load fine-tuned model for inference.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to saved model or HuggingFace repo\n",
    "        max_seq_length: Maximum sequence length\n",
    "        load_in_4bit: Whether to load in 4-bit\n",
    "\n",
    "    Returns:\n",
    "        Model and tokenizer\n",
    "    \"\"\"\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_path,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "    # Set to inference mode\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "class SemanticIDGenerator:\n",
    "    \"\"\"Generator for semantic IDs with optional constrained decoding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        num_quantizers: int = 4,\n",
    "        codebook_size: int = 256,\n",
    "        use_constrained_decoding: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the generator.\n",
    "\n",
    "        Args:\n",
    "            model: Fine-tuned model\n",
    "            tokenizer: Tokenizer with semantic ID tokens\n",
    "            num_quantizers: Number of RQ-VAE quantizers\n",
    "            codebook_size: Size of each codebook\n",
    "            use_constrained_decoding: Whether to use constrained decoding\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_quantizers = num_quantizers\n",
    "        self.codebook_size = codebook_size\n",
    "        self.use_constrained_decoding = use_constrained_decoding\n",
    "\n",
    "        # Build semantic token ID lookup\n",
    "        self.semantic_token_ids = [\n",
    "            tokenizer.convert_tokens_to_ids(f\"[SEM_{q}_{c}]\")\n",
    "            for q in range(num_quantizers)\n",
    "            for c in range(codebook_size)\n",
    "        ]\n",
    "        self.semantic_token_ids_set = set(self.semantic_token_ids)\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        self.system_prompt = (\n",
    "            \"You are a recommendation system. Given a user query, \"\n",
    "            \"output the semantic ID of the most relevant item. \"\n",
    "            \"Respond only with the semantic ID tokens.\"\n",
    "        )\n",
    "\n",
    "    def _get_allowed_tokens_for_position(self, position: int) -> list[int]:\n",
    "        \"\"\"Return allowed token IDs for each position in the semantic ID.\"\"\"\n",
    "        if position < self.num_quantizers:\n",
    "            # Only allow tokens from the correct quantizer level\n",
    "            start_idx = position * self.codebook_size\n",
    "            end_idx = start_idx + self.codebook_size\n",
    "            return self.semantic_token_ids[start_idx:end_idx]\n",
    "        else:\n",
    "            # After all quantizers, only allow EOS\n",
    "            return [self.eos_token_id]\n",
    "\n",
    "    def _prefix_allowed_tokens_fn(self, batch_id: int, input_ids) -> list[int]:\n",
    "        \"\"\"Constrained decoding: only allow valid semantic ID tokens in sequence.\"\"\"\n",
    "        generated = input_ids.tolist()\n",
    "        sem_count = sum(1 for tok_id in generated if tok_id in self.semantic_token_ids_set)\n",
    "        return self._get_allowed_tokens_for_position(sem_count)\n",
    "\n",
    "    def _extract_semantic_id(self, generated_text: str) -> str:\n",
    "        \"\"\"Extract semantic ID from generated text.\"\"\"\n",
    "        if \"<|assistant|>\" in generated_text:\n",
    "            semantic_id = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "        else:\n",
    "            semantic_id = generated_text.strip()\n",
    "\n",
    "        # Clean up EOS token if present\n",
    "        if self.tokenizer.eos_token:\n",
    "            semantic_id = semantic_id.replace(self.tokenizer.eos_token, \"\").strip()\n",
    "\n",
    "        return semantic_id\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        query: str,\n",
    "        temperature: float = 0.1,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate semantic ID for a query.\n",
    "\n",
    "        Args:\n",
    "            query: User query\n",
    "            temperature: Sampling temperature (ignored if constrained)\n",
    "\n",
    "        Returns:\n",
    "            Generated semantic ID string\n",
    "        \"\"\"\n",
    "        prompt = f\"<|system|>\\n{self.system_prompt}\\n<|user|>\\n{query}\\n<|assistant|>\\n\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        if self.use_constrained_decoding:\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.num_quantizers + 1,\n",
    "                do_sample=False,  # Greedy for constrained\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn,\n",
    "            )\n",
    "        else:\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=32,\n",
    "                temperature=temperature,\n",
    "                do_sample=temperature > 0,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        return self._extract_semantic_id(generated)\n",
    "\n",
    "    def generate_beam(\n",
    "        self,\n",
    "        query: str,\n",
    "        num_beams: int = 10,\n",
    "        num_return_sequences: int = 10,\n",
    "    ) -> list[tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Generate multiple semantic IDs using beam search.\n",
    "\n",
    "        Args:\n",
    "            query: User query\n",
    "            num_beams: Number of beams for beam search\n",
    "            num_return_sequences: Number of sequences to return\n",
    "\n",
    "        Returns:\n",
    "            List of (semantic_id, score) tuples, sorted by score (highest first)\n",
    "        \"\"\"\n",
    "        prompt = f\"<|system|>\\n{self.system_prompt}\\n<|user|>\\n{query}\\n<|assistant|>\\n\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=self.num_quantizers + 1,\n",
    "            num_beams=num_beams,\n",
    "            num_return_sequences=min(num_return_sequences, num_beams),\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            prefix_allowed_tokens_fn=self._prefix_allowed_tokens_fn if self.use_constrained_decoding else None,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            use_cache=False,  # Disable KV cache to fix beam search with LoRA/Unsloth\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        sequences = outputs.sequences\n",
    "        # Compute sequence scores (sum of log probs)\n",
    "        scores = outputs.sequences_scores if hasattr(outputs, 'sequences_scores') else [0.0] * len(sequences)\n",
    "\n",
    "        for seq, score in zip(sequences, scores):\n",
    "            generated = self.tokenizer.decode(seq, skip_special_tokens=False)\n",
    "            semantic_id = self._extract_semantic_id(generated)\n",
    "            score_val = score.item() if hasattr(score, 'item') else float(score)\n",
    "            results.append((semantic_id, score_val))\n",
    "\n",
    "        # Sort by score (highest first) and deduplicate\n",
    "        seen = set()\n",
    "        unique_results = []\n",
    "        for sem_id, score in sorted(results, key=lambda x: x[1], reverse=True):\n",
    "            if sem_id not in seen:\n",
    "                seen.add(sem_id)\n",
    "                unique_results.append((sem_id, score))\n",
    "\n",
    "        return unique_results\n",
    "\n",
    "    def __call__(self, query: str, **kwargs) -> str:\n",
    "        \"\"\"Shorthand for generate().\"\"\"\n",
    "        return self.generate(query, **kwargs)\n",
    "\n",
    "\n",
    "def generate_semantic_id(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    query: str,\n",
    "    max_new_tokens: int = 32,\n",
    "    temperature: float = 0.1,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate semantic ID for a query (simple version without constrained decoding).\n",
    "\n",
    "    For constrained decoding, use SemanticIDGenerator instead.\n",
    "\n",
    "    Args:\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Tokenizer\n",
    "        query: User query\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "\n",
    "    Returns:\n",
    "        Generated semantic ID string\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a recommendation system. Given a user query, \"\n",
    "        \"output the semantic ID of the most relevant item. \"\n",
    "        \"Respond only with the semantic ID tokens.\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{query}\\n<|assistant|>\\n\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode and extract semantic ID\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # Extract part after <|assistant|>\n",
    "    if \"<|assistant|>\" in generated:\n",
    "        semantic_id = generated.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        semantic_id = generated[len(prompt):].strip()\n",
    "\n",
    "    return semantic_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to inference mode and create generator with constrained decoding\n",
    "FastLanguageModel.for_inference(llm_model)\n",
    "\n",
    "from src.llm.finetune import SemanticIDGenerator\n",
    "\n",
    "generator = SemanticIDGenerator(\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    num_quantizers=config.num_quantizers,\n",
    "    codebook_size=config.codebook_size,\n",
    "    use_constrained_decoding=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Inference mode ready with constrained decoding\")\n",
    "print(f\"  Semantic tokens: {config.num_quantizers * config.codebook_size} total\")\n",
    "print(f\"  Output format: {config.num_quantizers} tokens (one per quantizer level)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries with beam search - show top 10 candidates and valid matches\n",
    "test_queries = [\n",
    "    \"News about stock market and business\",\n",
    "    \"Sports football game results\",\n",
    "    \"Technology and science discoveries\",\n",
    "    \"World news international politics\",\n",
    "]\n",
    "\n",
    "NUM_BEAMS = 10\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFERENCE TESTS (Beam Search)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_table = []\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    # Get top candidates via beam search\n",
    "    candidates = generator.generate_beam(query, num_beams=NUM_BEAMS, num_return_sequences=NUM_BEAMS)\n",
    "    \n",
    "    print(f\"\\nTop {len(candidates)} candidates:\")\n",
    "    valid_count = 0\n",
    "    for rank, (sem_id, score) in enumerate(candidates, 1):\n",
    "        if sem_id in semantic_to_item:\n",
    "            item_id = semantic_to_item[sem_id]\n",
    "            item = next(i for i in items if i[\"item_id\"] == item_id)\n",
    "            print(f\"  {rank}. ✓ {sem_id} (score: {score:.2f})\")\n",
    "            print(f\"     [{item['category']}] {item['title'][:55]}...\")\n",
    "            valid_count += 1\n",
    "            results_table.append([query, rank, sem_id, score, item['category'], item['title'][:50], True])\n",
    "        else:\n",
    "            print(f\"  {rank}. ✗ {sem_id} (score: {score:.2f}) - not in catalogue\")\n",
    "            results_table.append([query, rank, sem_id, score, \"\", \"\", False])\n",
    "    \n",
    "    print(f\"\\n  Valid matches: {valid_count}/{len(candidates)}\")\n",
    "\n",
    "# Log results to wandb\n",
    "wandb.log({\n",
    "    \"inference_results\": wandb.Table(\n",
    "        columns=[\"query\", \"rank\", \"semantic_id\", \"score\", \"category\", \"title\", \"valid\"],\n",
    "        data=results_table\n",
    "    )\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ All tests complete!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested:\n",
    "\n",
    "1. **RQ-VAE Training** - ✓ Model learns to encode items as discrete codes\n",
    "2. **Semantic ID Generation** - ✓ Each item gets a unique semantic ID\n",
    "3. **Training Data Creation** - ✓ Query → Semantic ID pairs generated\n",
    "4. **LLM Fine-tuning** - ✓ Model learns to predict semantic IDs\n",
    "5. **Inference** - ✓ Model generates semantic IDs for new queries\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "For production use:\n",
    "- Use larger model (Qwen3-4B or Ministral-3B)\n",
    "- Train for more epochs (3-5)\n",
    "- Use full catalogue\n",
    "- Deploy to Modal for serverless inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
