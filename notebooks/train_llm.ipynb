{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-tuning for Semantic ID Generation\n",
    "\n",
    "Fine-tune a small LLM to generate semantic IDs from user queries using a trained RQ-VAE model.\n",
    "\n",
    "**Environment:** RunPod Jupyter with GPU\n",
    "\n",
    "**Prerequisites:**\n",
    "- Trained RQ-VAE model (either as W&B artifact or local file)\n",
    "- Item catalogue (same one used for RQ-VAE training)\n",
    "\n",
    "**Training Stages:**\n",
    "1. **Stage 1**: Train only embedding layers for new semantic ID tokens (backbone frozen)\n",
    "2. **Stage 2**: LoRA fine-tuning on the full model using stage 1 checkpoint\n",
    "\n",
    "**Outputs:**\n",
    "- `checkpoints/llm_stage1/` - Stage 1 model (embeddings only)\n",
    "- `checkpoints/llm_stage2/` - Stage 2 model (full fine-tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to project root if needed\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if os.path.abspath(\".\").endswith(\"notebooks\"):\n",
    "    repo_root = os.path.abspath(\"..\")\n",
    "    print(f\"Current directory: {os.getcwd()}, changing to {repo_root}\")\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.insert(0, repo_root)\n",
    "    os.chdir(repo_root)\n",
    "else:\n",
    "    print(f\"Already in project root: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel  # Always import unsloth first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from dataclasses import replace\n",
    "\n",
    "from src.llm import (\n",
    "    train,\n",
    "    LLMTrainConfig,\n",
    "    LLMTrainResult,\n",
    "    SemanticIDGenerator,\n",
    "    load_finetuned_model,\n",
    ")\n",
    "\n",
    "print(\"Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All training parameters are consolidated into `LLMTrainConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 configuration - all parameters in one place\n",
    "stage1_config = LLMTrainConfig(\n",
    "    # RQ-VAE Model Source\n",
    "    # Option 1: Load from W&B artifact (recommended)\n",
    "    wandb_rqvae_artifact=\"rqvae-model:latest\",  # e.g., \"rqvae-model:v3\"\n",
    "    # Option 2: Load from local file\n",
    "    # rqvae_model_path=\"models/rqvae_model.pt\",\n",
    "    \n",
    "    # Catalogue and Embeddings (must match RQ-VAE training)\n",
    "    catalogue_path=\"data/mcf_articles.jsonl\",\n",
    "    catalogue_id_field=\"item_id\",\n",
    "    embedding_model=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    embeddings_cache_path=\"data/embeddings_mcf.pt\",\n",
    "    \n",
    "    # Query templates for training data generation\n",
    "    query_templates={\n",
    "        \"predict_semantic_id\": [\n",
    "            \"{title}\",\n",
    "            \"Find: {title}\",\n",
    "            \"Search for {title}\",\n",
    "            \"Recommend: {title}\",\n",
    "            \"Show me {title}\",\n",
    "            \"I want to read about {title}\",\n",
    "            \"Article about {title}\",\n",
    "        ],\n",
    "        \"predict_attribute\": [\n",
    "            \"What is the {field_name} for {semantic_id}?\",\n",
    "            \"Get {field_name} for {semantic_id}\",\n",
    "            \"{semantic_id} - what is the {field_name}?\",\n",
    "        ],\n",
    "    },\n",
    "    field_mapping={\"title\": \"title\", \"category\": \"category\"},\n",
    "    num_examples_per_item=5,\n",
    "    predict_semantic_id_ratio=0.8,\n",
    "    val_split=0.1,\n",
    "    \n",
    "    # Base LLM\n",
    "    # Options: \"unsloth/Qwen3-4B\", \"unsloth/Qwen3-1.7B\", \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "    base_model=\"HuggingFaceTB/SmolLM2-135M-Instruct\",\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    \n",
    "    # Stage 1: Embedding training (backbone frozen)\n",
    "    stage=1,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate=2e-4,\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.03,\n",
    "    \n",
    "    # Output\n",
    "    output_dir=\"checkpoints/llm_stage1\",\n",
    "    semantic_ids_output_path=\"data/semantic_ids.json\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    \n",
    "    # W&B configuration\n",
    "    wandb_project=\"semantic-id-recommender\",\n",
    "    wandb_run_name=\"llm-stage1\",\n",
    "    report_to=\"wandb\",\n",
    "    log_wandb_artifacts=True,\n",
    "    \n",
    "    # Test queries for evaluation callback\n",
    "    recommendation_test_queries=[\n",
    "        \"News about stock market and business\",\n",
    "        \"Sports football game results\",\n",
    "        \"Technology and science discoveries\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create output directories\n",
    "Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Stage 1 Configuration:\")\n",
    "print(f\"  RQ-VAE source: {stage1_config.wandb_rqvae_artifact or stage1_config.rqvae_model_path}\")\n",
    "print(f\"  Catalogue: {stage1_config.catalogue_path}\")\n",
    "print(f\"  Embedding model: {stage1_config.embedding_model}\")\n",
    "print(f\"  Base LLM: {stage1_config.base_model}\")\n",
    "print(f\"  Output: {stage1_config.output_dir}\")\n",
    "print(f\"  Effective batch size: {stage1_config.batch_size * stage1_config.gradient_accumulation_steps}\")\n",
    "print(f\"  W&B project: {stage1_config.wandb_project}\")\n",
    "print(f\"  Log W&B artifacts: {stage1_config.log_wandb_artifacts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(result: LLMTrainResult, test_queries: list[str] | None = None):\n",
    "    \"\"\"\n",
    "    Test a trained model with sample queries.\n",
    "    \n",
    "    Args:\n",
    "        result: LLMTrainResult from train()\n",
    "        test_queries: List of queries to test (uses defaults if None)\n",
    "    \"\"\"\n",
    "    if test_queries is None:\n",
    "        test_queries = [\n",
    "            \"News about stock market and business\",\n",
    "            \"Sports football game results\",\n",
    "            \"Technology and science discoveries\",\n",
    "            \"World news international politics\",\n",
    "        ]\n",
    "    \n",
    "    # Create semantic ID generator from trained model\n",
    "    generator = SemanticIDGenerator(\n",
    "        model=result.model,\n",
    "        tokenizer=result.tokenizer,\n",
    "        num_quantizers=result.semantic_id_mapping[\"config\"][\"num_quantizers\"],\n",
    "    )\n",
    "    \n",
    "    # Get semantic_id -> item mapping\n",
    "    semantic_to_item = result.semantic_id_mapping[\"semantic_to_item\"]\n",
    "    \n",
    "    print(\"Testing semantic ID generation:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        # Generate using beam search for multiple candidates\n",
    "        results = generator.generate_beam(query, num_beams=5, num_return_sequences=3)\n",
    "        \n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        for i, (sem_id, score) in enumerate(results[:3]):\n",
    "            item_id = semantic_to_item.get(sem_id)\n",
    "            if item_id:\n",
    "                print(f\"  {i+1}. [OK] {sem_id} (score: {score:.2f}) -> {item_id}\")\n",
    "            else:\n",
    "                print(f\"  {i+1}. [--] {sem_id} (score: {score:.2f}) -> (not in catalogue)\")\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 1: Embedding Training\n",
    "\n",
    "In stage 1, we:\n",
    "- Add new semantic ID tokens to the vocabulary\n",
    "- Freeze the entire backbone\n",
    "- Train only the input/output embedding layers\n",
    "\n",
    "This teaches the model to recognize and generate the new semantic ID tokens.\n",
    "\n",
    "The `train()` function handles the complete pipeline:\n",
    "1. Initialize W&B\n",
    "2. Load RQ-VAE model from artifact or local path\n",
    "3. Create semantic ID mapping for all catalogue items\n",
    "4. Prepare training data (query -> semantic ID pairs)\n",
    "5. Train the LLM\n",
    "6. Log artifacts to W&B\n",
    "7. Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 1 training\n",
    "stage1_result = train(stage1_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Stage 1 Training Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model saved to: {stage1_config.output_dir}\")\n",
    "print(f\"Semantic IDs saved to: {stage1_config.semantic_ids_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Stage 1 model\n",
    "print(\"\\n=== Stage 1 Model Test ===\")\n",
    "_ = test_model(stage1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory before Stage 2\n",
    "del stage1_result.model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared GPU memory for Stage 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 2: LoRA Fine-tuning\n",
    "\n",
    "In stage 2, we:\n",
    "- Load the stage 1 checkpoint (with trained embeddings)\n",
    "- Apply LoRA adapters to all linear layers\n",
    "- Fine-tune the model to generate semantic IDs from queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 configuration - copy from stage 1 and modify\n",
    "stage2_config = replace(\n",
    "    stage1_config,\n",
    "    \n",
    "    # Stage 2: LoRA fine-tuning\n",
    "    stage=2,\n",
    "    stage1_checkpoint=stage1_config.output_dir,  # Load from stage 1\n",
    "    # Or load from W&B artifact:\n",
    "    # wandb_stage1_artifact=\"llm-stage1:latest\",\n",
    "    \n",
    "    # LoRA settings\n",
    "    lora_r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    \n",
    "    # Training hyperparameters (can adjust for stage 2)\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Output\n",
    "    output_dir=\"checkpoints/llm_stage2\",\n",
    "    \n",
    "    # W&B\n",
    "    wandb_run_name=\"llm-stage2\",\n",
    ")\n",
    "\n",
    "print(\"Stage 2 Configuration:\")\n",
    "print(f\"  Stage 1 checkpoint: {stage2_config.stage1_checkpoint}\")\n",
    "print(f\"  LoRA rank: {stage2_config.lora_r}\")\n",
    "print(f\"  LoRA alpha: {stage2_config.lora_alpha}\")\n",
    "print(f\"  Learning rate: {stage2_config.learning_rate}\")\n",
    "print(f\"  Epochs: {stage2_config.num_train_epochs}\")\n",
    "print(f\"  Output: {stage2_config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 2 training\n",
    "stage2_result = train(stage2_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Stage 2 Training Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model saved to: {stage2_config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Stage 2 model\n",
    "print(\"\\n=== Stage 2 Model Test ===\")\n",
    "generator = test_model(stage2_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model Later\n",
    "\n",
    "To load the trained model in a new session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load model from local checkpoint\n",
    "# model, tokenizer = load_finetuned_model(\"checkpoints/llm_stage2\")\n",
    "# generator = SemanticIDGenerator(model, tokenizer, num_quantizers=3)\n",
    "# semantic_id = generator.generate(\"Recommend me an article about cooking\")\n",
    "# print(f\"Generated: {semantic_id}\")\n",
    "\n",
    "# Example: Load model from W&B artifact\n",
    "# import wandb\n",
    "# wandb.init(project=\"semantic-id-recommender\", job_type=\"inference\")\n",
    "# artifact = wandb.use_artifact(\"llm-stage2:latest\")\n",
    "# artifact_dir = artifact.download()\n",
    "# model, tokenizer = load_finetuned_model(artifact_dir)\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(f\"\\nFinal model: {stage2_config.output_dir}\")\n",
    "print(f\"\\nTo use the model:\")\n",
    "print(f\"  from src.llm import load_finetuned_model, SemanticIDGenerator\")\n",
    "print(f\"  model, tokenizer = load_finetuned_model('{stage2_config.output_dir}')\")\n",
    "print(f\"  generator = SemanticIDGenerator(model, tokenizer, num_quantizers=3)\")\n",
    "print(f\"  semantic_id = generator.generate('your query here')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
