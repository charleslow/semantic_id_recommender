{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LLM Fine-tuning for Semantic ID Generation\n",
    "\n",
    "Fine-tune a small LLM to generate semantic IDs from user queries using a trained RQ-VAE model.\n",
    "\n",
    "**Environment:** RunPod Jupyter with GPU\n",
    "\n",
    "**Prerequisites:**\n",
    "- Trained RQ-VAE model (`models/rqvae_model.pt`)\n",
    "- Semantic ID mappings (`data/semantic_ids.json`)\n",
    "- Item catalogue (`data/mcf_articles.jsonl`)\n",
    "\n",
    "**Training Stages:**\n",
    "1. **Stage 1**: Train only embedding layers for new semantic ID tokens (backbone frozen)\n",
    "2. **Stage 2**: LoRA fine-tuning on the full model using stage 1 checkpoint\n",
    "\n",
    "**Outputs:**\n",
    "- `checkpoints/llm_stage1/` - Stage 1 model (embeddings only)\n",
    "- `checkpoints/llm_stage2/` - Stage 2 model (full fine-tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to project root if needed\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if os.path.abspath(\".\").endswith(\"notebooks\"):\n",
    "    repo_root = os.path.abspath(\"..\")\n",
    "    print(f\"Current directory: {os.getcwd()}, changing to {repo_root}\")\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.insert(0, repo_root)\n",
    "    os.chdir(repo_root)\n",
    "else:\n",
    "    print(f\"Already in project root: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel  # Always import unsloth first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "from src.llm import (\n",
    "    FinetuneConfig,\n",
    "    finetune_model,\n",
    "    prepare_training_data,\n",
    "    SemanticIDDataset,\n",
    "    SemanticIDGenerator,\n",
    "    load_finetuned_model,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CATALOGUE_PATH = \"data/mcf_articles.jsonl\"\n",
    "SEMANTIC_IDS_PATH = \"data/semantic_ids.json\"\n",
    "TRAIN_DATA_PATH = \"data/llm_train.jsonl\"\n",
    "VAL_DATA_PATH = \"data/llm_val.jsonl\"\n",
    "\n",
    "# Model checkpoints\n",
    "STAGE1_OUTPUT_DIR = \"checkpoints/llm_stage1\"\n",
    "STAGE2_OUTPUT_DIR = \"checkpoints/llm_stage2\"\n",
    "\n",
    "# Base model - choose based on your GPU memory\n",
    "# Options: \"unsloth/Qwen3-4B\", \"unsloth/Qwen3-1.7B\", \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "BASE_MODEL = \"unsloth/Qwen3-4B\"\n",
    "\n",
    "# Must match RQ-VAE configuration\n",
    "NUM_QUANTIZERS = 3\n",
    "CODEBOOK_SIZE = 64\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "# Stage 1: Embedding training\n",
    "STAGE1_EPOCHS = 3\n",
    "STAGE1_LR = 2e-4\n",
    "\n",
    "# Stage 2: LoRA fine-tuning\n",
    "STAGE2_EPOCHS = 3\n",
    "STAGE2_LR = 2e-4\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "\n",
    "# W&B Artifact logging - set to True to save model checkpoints as W&B artifacts\n",
    "LOG_WANDB_ARTIFACTS = False\n",
    "\n",
    "# Create output directories\n",
    "Path(STAGE1_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(STAGE2_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  Semantic ID tokens: {NUM_QUANTIZERS} quantizers x {CODEBOOK_SIZE} codes\")\n",
    "print(f\"  Stage 1 output: {STAGE1_OUTPUT_DIR}\")\n",
    "print(f\"  Stage 2 output: {STAGE2_OUTPUT_DIR}\")\n",
    "print(f\"  Log W&B artifacts: {LOG_WANDB_ARTIFACTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Verify Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "print(\"=== Environment Variables ===\")\n",
    "if HF_TOKEN:\n",
    "    print(f\"HF_TOKEN is set (length: {len(HF_TOKEN)})\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not set - some models may not be accessible\")\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    print(f\"WANDB_API_KEY is set (length: {len(WANDB_API_KEY)})\")\n",
    "else:\n",
    "    print(\"WANDB_API_KEY not set - Weights & Biases logging disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Initialize Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "WANDB_PROJECT = \"semantic-id-recommender\"\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    # Don't initialize here - let the trainer handle it\n",
    "    # Just verify we can connect\n",
    "    wandb.login()\n",
    "    print(f\"Weights & Biases ready\")\n",
    "    print(f\"  Project: {WANDB_PROJECT}\")\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    print(\"Wandb logging disabled\")\n",
    "    REPORT_TO = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Load Semantic ID Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load semantic ID mappings from RQ-VAE training\n",
    "with open(SEMANTIC_IDS_PATH) as f:\n",
    "    semantic_mapping = json.load(f)\n",
    "\n",
    "item_to_semantic = semantic_mapping[\"item_to_semantic\"]\n",
    "semantic_to_item = semantic_mapping[\"semantic_to_item\"]\n",
    "rqvae_config = semantic_mapping.get(\"config\", {})\n",
    "\n",
    "print(f\"Loaded {len(item_to_semantic)} item -> semantic ID mappings\")\n",
    "print(f\"Loaded {len(semantic_to_item)} semantic ID -> item mappings\")\n",
    "print(f\"\\nRQ-VAE config from mapping:\")\n",
    "for k, v in rqvae_config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Verify config matches\n",
    "if rqvae_config:\n",
    "    assert rqvae_config.get(\"num_quantizers\") == NUM_QUANTIZERS, \"NUM_QUANTIZERS mismatch!\"\n",
    "    assert rqvae_config.get(\"codebook_size\") == CODEBOOK_SIZE, \"CODEBOOK_SIZE mismatch!\"\n",
    "    print(\"\\nConfiguration matches RQ-VAE settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load catalogue items for recommendation test callback\n",
    "items = []\n",
    "with open(CATALOGUE_PATH) as f:\n",
    "    for line in f:\n",
    "        items.append(json.loads(line))\n",
    "\n",
    "# Create semantic_id -> item info mapping for callback\n",
    "semantic_id_to_item = {}\n",
    "for item in items:\n",
    "    item_id = item.get(\"item_id\", item.get(\"id\", \"\"))\n",
    "    if str(item_id) in item_to_semantic:\n",
    "        sem_id = item_to_semantic[str(item_id)][\"semantic_id\"]\n",
    "        semantic_id_to_item[sem_id] = {\n",
    "            \"item_id\": item_id,\n",
    "            \"title\": item.get(\"title\", \"\"),\n",
    "            \"category\": item.get(\"category\", \"\"),\n",
    "        }\n",
    "\n",
    "print(f\"\\nLoaded {len(items)} catalogue items\")\n",
    "print(f\"Created {len(semantic_id_to_item)} semantic_id -> item mappings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 6. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query templates for training\n",
    "# Customize these based on your use case\n",
    "QUERY_TEMPLATES = {\n",
    "    \"predict_semantic_id\": [\n",
    "        \"What is the semantic ID of {title}?\",\n",
    "        \"Find the semantic ID for {title}\",\n",
    "        \"Semantic ID for: {title}\",\n",
    "        \"Recommend something like {title}. What is its semantic ID?\",\n",
    "        \"{title} - what's the semantic ID?\",\n",
    "        \"Item: {title}. Semantic ID?\",\n",
    "        \"Get semantic ID: {title}\",\n",
    "    ],\n",
    "    \"predict_attribute\": [\n",
    "        \"What is the {field_name} for semantic ID {semantic_id}?\",\n",
    "        \"For semantic ID {semantic_id}, what is the {field_name}?\",\n",
    "        \"Get {field_name} for {semantic_id}\",\n",
    "        \"{semantic_id} - what's the {field_name}?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Field mapping: template placeholder -> actual field name in catalogue\n",
    "# Adjust based on your catalogue structure\n",
    "FIELD_MAPPING = {\n",
    "    \"title\": \"title\",\n",
    "    \"category\": \"category\",\n",
    "}\n",
    "\n",
    "# ID field name in your catalogue\n",
    "ID_FIELD = \"item_id\"  # or \"id\" depending on your catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and validation datasets\n",
    "train_dataset, val_dataset = prepare_training_data(\n",
    "    catalogue_path=CATALOGUE_PATH,\n",
    "    semantic_ids_path=SEMANTIC_IDS_PATH,\n",
    "    output_train_path=TRAIN_DATA_PATH,\n",
    "    output_val_path=VAL_DATA_PATH,\n",
    "    query_templates=QUERY_TEMPLATES,\n",
    "    field_mapping=FIELD_MAPPING,\n",
    "    id_field=ID_FIELD,\n",
    "    num_examples_per_item=5,\n",
    "    val_split=0.1,\n",
    "    predict_semantic_id_ratio=0.8,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview some training examples\n",
    "print(\"Sample training examples:\")\n",
    "for i in range(min(3, len(train_dataset))):\n",
    "    example = train_dataset[i]\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    for msg in example[\"messages\"]:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"][:100] + \"...\" if len(msg[\"content\"]) > 100 else msg[\"content\"]\n",
    "        print(f\"{role}: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 7. Stage 1: Embedding Training\n",
    "\n",
    "In stage 1, we:\n",
    "- Add new semantic ID tokens to the vocabulary\n",
    "- Freeze the entire backbone\n",
    "- Train only the input/output embedding layers\n",
    "\n",
    "This teaches the model to recognize and generate the new semantic ID tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries for the recommendation callback\n",
    "TEST_QUERIES = [\n",
    "    \"Recommend me an article about cooking\",\n",
    "    \"I want to read something about technology\",\n",
    "    \"Find me a sports article\",\n",
    "]\n",
    "\n",
    "# Stage 1 configuration\n",
    "stage1_config = FinetuneConfig(\n",
    "    # Model\n",
    "    base_model=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,\n",
    "    \n",
    "    # Semantic ID settings (must match RQ-VAE)\n",
    "    num_quantizers=NUM_QUANTIZERS,\n",
    "    codebook_size=CODEBOOK_SIZE,\n",
    "    \n",
    "    # Training\n",
    "    stage=1,  # Embedding training only\n",
    "    learning_rate=STAGE1_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    num_train_epochs=STAGE1_EPOCHS,\n",
    "    warmup_ratio=0.03,\n",
    "    \n",
    "    # Logging and saving\n",
    "    output_dir=STAGE1_OUTPUT_DIR,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    report_to=REPORT_TO,\n",
    "    \n",
    "    # W&B artifact logging\n",
    "    log_wandb_artifacts=LOG_WANDB_ARTIFACTS,\n",
    "    \n",
    "    # Callbacks\n",
    "    recommendation_test_queries=TEST_QUERIES,\n",
    "    semantic_id_to_item=semantic_id_to_item,\n",
    ")\n",
    "\n",
    "print(\"Stage 1 Configuration:\")\n",
    "print(f\"  Base model: {stage1_config.base_model}\")\n",
    "print(f\"  Learning rate: {stage1_config.learning_rate}\")\n",
    "print(f\"  Epochs: {stage1_config.num_train_epochs}\")\n",
    "print(f\"  Effective batch size: {stage1_config.batch_size * stage1_config.gradient_accumulation_steps}\")\n",
    "print(f\"  Log W&B artifacts: {stage1_config.log_wandb_artifacts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 1 training\n",
    "print(\"Starting Stage 1: Embedding Training\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "stage1_model, stage1_tokenizer = finetune_model(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    config=stage1_config,\n",
    ")\n",
    "\n",
    "print(\"\\nStage 1 complete!\")\n",
    "print(f\"Model saved to: {STAGE1_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify stage 1 model\n",
    "print(\"Stage 1 model info:\")\n",
    "print(f\"  Vocabulary size: {len(stage1_tokenizer)}\")\n",
    "\n",
    "# Check that semantic ID tokens were added\n",
    "test_token = f\"[SEM_0_0]\"\n",
    "token_id = stage1_tokenizer.convert_tokens_to_ids(test_token)\n",
    "print(f\"  Test token '{test_token}' -> ID: {token_id}\")\n",
    "\n",
    "# Count trainable parameters in stage 1\n",
    "trainable_params = sum(p.numel() for p in stage1_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in stage1_model.parameters())\n",
    "print(f\"  Trainable params: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 8. Stage 2: LoRA Fine-tuning\n",
    "\n",
    "In stage 2, we:\n",
    "- Load the stage 1 checkpoint (with trained embeddings)\n",
    "- Apply LoRA adapters to all linear layers\n",
    "- Fine-tune the model to generate semantic IDs from queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up stage 1 model from GPU memory\n",
    "del stage1_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared GPU memory for stage 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 configuration\n",
    "stage2_config = FinetuneConfig(\n",
    "    # Model - load from stage 1 checkpoint\n",
    "    base_model=BASE_MODEL,  # Not used when stage1_checkpoint is set\n",
    "    stage1_checkpoint=STAGE1_OUTPUT_DIR,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,\n",
    "    \n",
    "    # Semantic ID settings (must match RQ-VAE)\n",
    "    num_quantizers=NUM_QUANTIZERS,\n",
    "    codebook_size=CODEBOOK_SIZE,\n",
    "    \n",
    "    # LoRA settings\n",
    "    lora_r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.05,\n",
    "    \n",
    "    # Training\n",
    "    stage=2,  # LoRA fine-tuning\n",
    "    learning_rate=STAGE2_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    num_train_epochs=STAGE2_EPOCHS,\n",
    "    warmup_ratio=0.03,\n",
    "    \n",
    "    # Logging and saving\n",
    "    output_dir=STAGE2_OUTPUT_DIR,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    report_to=REPORT_TO,\n",
    "    \n",
    "    # W&B artifact logging\n",
    "    log_wandb_artifacts=LOG_WANDB_ARTIFACTS,\n",
    "    \n",
    "    # Callbacks\n",
    "    recommendation_test_queries=TEST_QUERIES,\n",
    "    semantic_id_to_item=semantic_id_to_item,\n",
    ")\n",
    "\n",
    "print(\"Stage 2 Configuration:\")\n",
    "print(f\"  Stage 1 checkpoint: {stage2_config.stage1_checkpoint}\")\n",
    "print(f\"  LoRA rank: {stage2_config.lora_r}\")\n",
    "print(f\"  LoRA alpha: {stage2_config.lora_alpha}\")\n",
    "print(f\"  Learning rate: {stage2_config.learning_rate}\")\n",
    "print(f\"  Epochs: {stage2_config.num_train_epochs}\")\n",
    "print(f\"  Log W&B artifacts: {stage2_config.log_wandb_artifacts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 2 training\n",
    "print(\"Starting Stage 2: LoRA Fine-tuning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "stage2_model, stage2_tokenizer = finetune_model(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    config=stage2_config,\n",
    ")\n",
    "\n",
    "print(\"\\nStage 2 complete!\")\n",
    "print(f\"Model saved to: {STAGE2_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 9. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "model, tokenizer = load_finetuned_model(STAGE2_OUTPUT_DIR)\n",
    "\n",
    "# Create semantic ID generator\n",
    "generator = SemanticIDGenerator(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    num_quantizers=NUM_QUANTIZERS,\n",
    ")\n",
    "\n",
    "print(\"Model loaded for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample queries\n",
    "test_queries = [\n",
    "    \"Recommend me an article about cooking\",\n",
    "    \"I want to read something about technology\",\n",
    "    \"Find me a sports article\",\n",
    "    \"What article should I read about health?\",\n",
    "]\n",
    "\n",
    "print(\"Testing semantic ID generation:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for query in test_queries:\n",
    "    semantic_id = generator.generate(query)\n",
    "    \n",
    "    # Look up the item if we have a mapping\n",
    "    item_info = semantic_id_to_item.get(semantic_id, {})\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"  Semantic ID: {semantic_id}\")\n",
    "    if item_info:\n",
    "        print(f\"  Item: {item_info.get('title', 'Unknown')}\")\n",
    "        print(f\"  Category: {item_info.get('category', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"  (No matching item found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 10. Save Final Model\n",
    "\n",
    "Optionally merge LoRA weights and save as a standalone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model info for later use\n",
    "model_info = {\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"num_quantizers\": NUM_QUANTIZERS,\n",
    "    \"codebook_size\": CODEBOOK_SIZE,\n",
    "    \"stage1_checkpoint\": STAGE1_OUTPUT_DIR,\n",
    "    \"stage2_checkpoint\": STAGE2_OUTPUT_DIR,\n",
    "    \"training_config\": {\n",
    "        \"stage1_epochs\": STAGE1_EPOCHS,\n",
    "        \"stage1_lr\": STAGE1_LR,\n",
    "        \"stage2_epochs\": STAGE2_EPOCHS,\n",
    "        \"stage2_lr\": STAGE2_LR,\n",
    "        \"lora_r\": LORA_R,\n",
    "        \"lora_alpha\": LORA_ALPHA,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{STAGE2_OUTPUT_DIR}/model_info.json\", \"w\") as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"Model info saved to {STAGE2_OUTPUT_DIR}/model_info.json\")\n",
    "\n",
    "# Note: W&B artifacts are now logged automatically by finetune_model\n",
    "# when log_wandb_artifacts=True in FinetuneConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log final metrics to wandb\n",
    "if WANDB_API_KEY and wandb.run:\n",
    "    wandb.log({\n",
    "        \"final/base_model\": BASE_MODEL,\n",
    "        \"final/num_quantizers\": NUM_QUANTIZERS,\n",
    "        \"final/codebook_size\": CODEBOOK_SIZE,\n",
    "        \"final/train_examples\": len(train_dataset),\n",
    "        \"final/val_examples\": len(val_dataset),\n",
    "    })\n",
    "    wandb.finish()\n",
    "    print(\"Logged final metrics to wandb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## 11. Load Model from W&B Artifacts (Optional)\n",
    "\n",
    "If you've logged models as W&B artifacts, you can load them in future sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load model from W&B artifact\n",
    "# Uncomment and modify the following to load a model from W&B\n",
    "\n",
    "# # Initialize wandb (if not already)\n",
    "# wandb.init(project=WANDB_PROJECT, job_type=\"inference\")\n",
    "# \n",
    "# # Download artifact - use \"latest\", \"best\", or a specific version like \"v0\"\n",
    "# artifact = wandb.use_artifact(\"llm-stage2:latest\")\n",
    "# artifact_dir = artifact.download()\n",
    "# \n",
    "# # Load the model\n",
    "# model, tokenizer = load_finetuned_model(artifact_dir)\n",
    "# generator = SemanticIDGenerator(model, tokenizer, num_quantizers=NUM_QUANTIZERS)\n",
    "# \n",
    "# # Use the model\n",
    "# semantic_id = generator.generate(\"Recommend me an article about cooking\")\n",
    "# print(f\"Generated semantic ID: {semantic_id}\")\n",
    "# \n",
    "# # Check artifact metadata\n",
    "# print(f\"\\nArtifact metadata:\")\n",
    "# for k, v in artifact.metadata.items():\n",
    "#     print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"To load a model from W&B artifacts, uncomment the code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## 12. Upload to HuggingFace Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to HuggingFace Hub\n",
    "# Uncomment and modify the following to upload your model\n",
    "\n",
    "# REPO_ID = \"your-username/semantic-id-llm\"  # Change this!\n",
    "# \n",
    "# if HF_TOKEN:\n",
    "#     stage2_model.push_to_hub(REPO_ID, token=HF_TOKEN)\n",
    "#     stage2_tokenizer.push_to_hub(REPO_ID, token=HF_TOKEN)\n",
    "#     print(f\"Model uploaded to: https://huggingface.co/{REPO_ID}\")\n",
    "# else:\n",
    "#     print(\"HF_TOKEN not set - cannot upload to Hub\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(f\"\\nFinal model saved to: {STAGE2_OUTPUT_DIR}\")\n",
    "print(\"\\nTo use the model:\")\n",
    "print(\"  from src.llm import load_finetuned_model, SemanticIDGenerator\")\n",
    "print(f\"  model, tokenizer = load_finetuned_model('{STAGE2_OUTPUT_DIR}')\")\n",
    "print(f\"  generator = SemanticIDGenerator(model, tokenizer, num_quantizers={NUM_QUANTIZERS})\")\n",
    "print(\"  semantic_id = generator.generate('your query here')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
