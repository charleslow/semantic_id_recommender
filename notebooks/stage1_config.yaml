# Stage 1: Embedding Training Configuration
# Train only embedding layers for new semantic ID tokens (backbone frozen)

# RQ-VAE Model Source
# Option 1: Load from W&B artifact (recommended)
wandb_rqvae_artifact: "rqvae-model:latest"
# Option 2: Load from local file
# rqvae_model_path: "models/rqvae_model.pt"

# Catalogue and Embeddings (must match RQ-VAE training)
catalogue_path: "data/mcf_articles.jsonl"
catalogue_id_field: "item_id"
embedding_model: "Qwen/Qwen3-Embedding-0.6B"
embeddings_cache_path: "data/embeddings_mcf.pt"

# Query templates for training data generation
query_templates:
  predict_semantic_id:
    - "{title}"
    - "Find: {title}"
    - "Search for {title}"
    - "Recommend: {title}"
    - "Show me {title}"
    - "I want to read about {title}"
    - "Article about {title}"
  predict_attribute:
    - "What is the {field_name} for {semantic_id}?"
    - "Get {field_name} for {semantic_id}"
    - "{semantic_id} - what is the {field_name}?"

field_mapping:
  title: "title"
  category: "category"

num_examples_per_item: 5
predict_semantic_id_ratio: 0.8
val_split: 0.1

# Base LLM
# Options: "unsloth/Qwen3-4B", "unsloth/Qwen3-1.7B", "HuggingFaceTB/SmolLM2-1.7B-Instruct"
base_model: "HuggingFaceTB/SmolLM2-135M-Instruct"
max_seq_length: 512
load_in_4bit: true

# Stage 1: Embedding training (backbone frozen)
stage: 1

# Training hyperparameters
learning_rate: 2.0e-4
batch_size: 4
gradient_accumulation_steps: 4
num_train_epochs: 3
warmup_ratio: 0.03

# Output
output_dir: "checkpoints/llm_stage1"
semantic_ids_output_path: "data/semantic_ids.json"

# Logging
logging_steps: 10
save_strategy: "epoch"
eval_steps: 100

# W&B configuration
wandb_project: "semantic-id-recommender"
wandb_run_name: "llm-stage1"
report_to: "wandb"
log_wandb_artifacts: true

# Test queries for evaluation callback
recommendation_test_queries:
  - "News about stock market and business"
  - "Sports football game results"
  - "Technology and science discoveries"
